{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Kaggle Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# the data root for kaggle submission\n",
    "# root = '/kaggle/input/rsna-2023-abdominal-trauma-detection'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Local Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "root = '/mnt/d/kaggle/rsna-2023-abdominal-trauma-detection'\n",
    "%matplotlib notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.image\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import dotenv\n",
    "import torch\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.nn.functional import silu, sigmoid, softmax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_image_root = f'{root}/train_images'\n",
    "train_data_path = f'{root}/train.csv'\n",
    "scan_path = f'{root}/train_series_meta.csv'\n",
    "injuries_path = f'{root}/image_level_labels.csv'\n",
    "tag_path = f'{root}/train_dicom_tags.parquet'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = pd.read_csv(train_data_path)\n",
    "scans = pd.read_csv(scan_path)\n",
    "injuries = pd.read_csv(injuries_path)\n",
    "tags = pd.read_parquet(tag_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    TRAIN_SPLIT = 0.95\n",
    "    LABELS = [\n",
    "        \"bowel_injury\", \"extravasation_injury\",\n",
    "        \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n",
    "        \"liver_healthy\", \"liver_low\", \"liver_high\",\n",
    "        \"spleen_healthy\", \"spleen_low\", \"spleen_high\",\n",
    "    ]\n",
    "    BATCH_SIZE = 128\n",
    "    IMAGE_SIZE = 512\n",
    "    NUM_EPOCHS = 10\n",
    "    VAL_BATCH_SIZE=128\n",
    "    BATCHES_PER_EVALUATION = 32\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choosing the Training & Validation Images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "label_by_patient = labels.groupby(['patient_id'])\n",
    "def id_to_labels(row):\n",
    "    return label_by_patient.get_group(row['patient_id']).squeeze()\n",
    "# The injuries give us \"interesting\" images to use\n",
    "data = pd.concat((injuries[['patient_id']].apply(id_to_labels, axis=1), injuries[['series_id', 'instance_number']]), axis=1)\n",
    "data['image_path'] = train_image_root + '/' + data['patient_id'].astype(str) + '/' + data['series_id'].astype(str) + '/' + data['instance_number'].astype(str) + \".dcm\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   patient_id  bowel_healthy  bowel_injury  extravasation_healthy  \\\n0       10004              1             0                      0   \n1       10004              1             0                      0   \n2       10004              1             0                      0   \n3       10004              1             0                      0   \n4       10004              1             0                      0   \n\n   extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n0                     1               0           1            0   \n1                     1               0           1            0   \n2                     1               0           1            0   \n3                     1               0           1            0   \n4                     1               0           1            0   \n\n   liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n0              1          0           0               0           0   \n1              1          0           0               0           0   \n2              1          0           0               0           0   \n3              1          0           0               0           0   \n4              1          0           0               0           0   \n\n   spleen_high  any_injury  series_id  instance_number  \\\n0            1           1      21057              362   \n1            1           1      21057              363   \n2            1           1      21057              364   \n3            1           1      21057              365   \n4            1           1      21057              366   \n\n                                          image_path  \n0  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n1  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n2  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n3  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n4  /kaggle/input/rsna-2023-abdominal-trauma-detec...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>bowel_healthy</th>\n      <th>bowel_injury</th>\n      <th>extravasation_healthy</th>\n      <th>extravasation_injury</th>\n      <th>kidney_healthy</th>\n      <th>kidney_low</th>\n      <th>kidney_high</th>\n      <th>liver_healthy</th>\n      <th>liver_low</th>\n      <th>liver_high</th>\n      <th>spleen_healthy</th>\n      <th>spleen_low</th>\n      <th>spleen_high</th>\n      <th>any_injury</th>\n      <th>series_id</th>\n      <th>instance_number</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>362</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>363</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>364</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>365</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>366</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# TRAIN_SPLIT of the patients will be used for training, the rest for validation\n",
    "train_people = pd.Series(data['patient_id'].unique()).sample(frac=config.TRAIN_SPLIT)\n",
    "train = data.loc[data['patient_id'].isin(train_people)]\n",
    "val = data.loc[~data['patient_id'].isin(train_people)]\n",
    "\n",
    "train.set_index(np.asarray(range(len(train))), inplace=True)\n",
    "val.set_index(np.asarray(range(len(val))), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   patient_id  bowel_healthy  bowel_injury  extravasation_healthy  \\\n0       10004              1             0                      0   \n1       10004              1             0                      0   \n2       10004              1             0                      0   \n3       10004              1             0                      0   \n4       10004              1             0                      0   \n\n   extravasation_injury  kidney_healthy  kidney_low  kidney_high  \\\n0                     1               0           1            0   \n1                     1               0           1            0   \n2                     1               0           1            0   \n3                     1               0           1            0   \n4                     1               0           1            0   \n\n   liver_healthy  liver_low  liver_high  spleen_healthy  spleen_low  \\\n0              1          0           0               0           0   \n1              1          0           0               0           0   \n2              1          0           0               0           0   \n3              1          0           0               0           0   \n4              1          0           0               0           0   \n\n   spleen_high  any_injury  series_id  instance_number  \\\n0            1           1      21057              362   \n1            1           1      21057              363   \n2            1           1      21057              364   \n3            1           1      21057              365   \n4            1           1      21057              366   \n\n                                          image_path  \n0  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n1  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n2  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n3  /kaggle/input/rsna-2023-abdominal-trauma-detec...  \n4  /kaggle/input/rsna-2023-abdominal-trauma-detec...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>bowel_healthy</th>\n      <th>bowel_injury</th>\n      <th>extravasation_healthy</th>\n      <th>extravasation_injury</th>\n      <th>kidney_healthy</th>\n      <th>kidney_low</th>\n      <th>kidney_high</th>\n      <th>liver_healthy</th>\n      <th>liver_low</th>\n      <th>liver_high</th>\n      <th>spleen_healthy</th>\n      <th>spleen_low</th>\n      <th>spleen_high</th>\n      <th>any_injury</th>\n      <th>series_id</th>\n      <th>instance_number</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>362</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>363</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>364</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>365</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10004</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>21057</td>\n      <td>366</td>\n      <td>/kaggle/input/rsna-2023-abdominal-trauma-detec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Declare Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "image_augment = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(config.IMAGE_SIZE, config.IMAGE_SIZE), scale=(0.8, 1.0), antialias=True),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    ds = pydicom.dcmread(path)\n",
    "    return ds.pixel_array\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, source: pd.DataFrame):\n",
    "        self.image_paths = source['image_path']\n",
    "        # 0/1 kidney injury, 0/1 fluid injury, kidney health (3 vals), liver health (3 vals), spleen health (3 vals)\n",
    "        self.labels = torch.from_numpy(source[config.LABELS].to_numpy().astype('float32'))\n",
    "        self.source = source\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.from_numpy(np.expand_dims(read_image(self.image_paths[idx]).astype('float32'), 0))\n",
    "        image -= image.min()\n",
    "        image /= image.max()\n",
    "\n",
    "        image = torch.repeat_interleave(image, 3, dim=0)\n",
    "        image = image_augment(image)\n",
    "        # print(image.min(), image.max(), image.shape)\n",
    "        if image.shape != (3, config.IMAGE_SIZE, config.IMAGE_SIZE):\n",
    "            raise Exception(f'Image at {self.image_paths[idx]} has shape {image.shape}')\n",
    "        label = (self.labels[idx][0:1], self.labels[idx][1:2], self.labels[idx][2:5], self.labels[idx][5:8], self.labels[idx][8:11])\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_ds = MedicalDataset(train)\n",
    "val_ds = MedicalDataset(val)\n",
    "train_dl = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=config.VAL_BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Declare Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(*(list(resnet50(weights=ResNet50_Weights.DEFAULT).children())[:-1]))\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.necks = nn.ModuleList([nn.Linear(2048, 32) for _ in range(5)])\n",
    "        self.bowel = nn.Linear(32, 1)\n",
    "        self.fluid = nn.Linear(32, 1)\n",
    "        self.kidney = nn.Linear(32, 3)\n",
    "        self.liver = nn.Linear(32, 3)\n",
    "        self.spleen = nn.Linear(32, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(self.backbone(x), start_dim=1)\n",
    "        # bowel, fluid, kidney, liver, spleen\n",
    "        necks = [silu(self.necks[i](x)) for i in range(5)]\n",
    "\n",
    "        bowel = torch.flatten(sigmoid(self.bowel(necks[0])), start_dim=1)\n",
    "        fluid = torch.flatten(sigmoid(self.fluid(necks[1])), start_dim=1)\n",
    "        kidney = softmax(torch.flatten(self.kidney(necks[2]), start_dim=1), dim=1)\n",
    "        liver = softmax(torch.flatten(self.liver(necks[3]), start_dim=1), dim=1)\n",
    "        spleen = softmax(torch.flatten(self.spleen(necks[4]), start_dim=1), dim=1)\n",
    "        return bowel, fluid, kidney, liver, spleen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "classifier = Classifier().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def calc_loss(output, target): # tuples 4D batched tensors please!\n",
    "    bce = nn.BCELoss()\n",
    "    cross = nn.CrossEntropyLoss()\n",
    "    loss = bce(output[0], target[0]) + bce(output[1], target[1]) + cross(output[2], target[2]) + cross(output[3], target[3]) + cross(output[4], target[4])\n",
    "    return loss\n",
    "optimizer = torch.optim.Adam(classifier.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 0 batch 0 loss: 4.661365985870361 (cur batch: 4.661365985870361)\nEpoch 0 batch 1 loss: 4.55441951751709 (cur batch: 4.447473049163818)\nEpoch 0 batch 2 loss: 4.452319145202637 (cur batch: 4.2481184005737305)\nEpoch 0 batch 3 loss: 4.364541530609131 (cur batch: 4.101208686828613)\nEpoch 0 batch 4 loss: 4.298175239562989 (cur batch: 4.032710075378418)\nEpoch 0 batch 5 loss: 4.207996924718221 (cur batch: 3.7571053504943848)\nEpoch 0 batch 6 loss: 4.137872219085693 (cur batch: 3.7171239852905273)\nEpoch 0 batch 7 loss: 4.078884929418564 (cur batch: 3.6659739017486572)\nEpoch 0 batch 8 loss: 4.03549697664049 (cur batch: 3.6883933544158936)\nEpoch 0 batch 9 loss: 3.9884081363677977 (cur batch: 3.564608573913574)\nEpoch 0 batch 10 loss: 3.9481254490939053 (cur batch: 3.5452985763549805)\nEpoch 0 batch 11 loss: 3.9284846782684326 (cur batch: 3.7124361991882324)\nEpoch 0 batch 12 loss: 3.8989008389986477 (cur batch: 3.5438947677612305)\nEpoch 0 batch 13 loss: 3.867074183055333 (cur batch: 3.4533276557922363)\nEpoch 0 batch 14 loss: 3.849371147155762 (cur batch: 3.6015286445617676)\nEpoch 0 batch 15 loss: 3.824675276875496 (cur batch: 3.454237222671509)\nEpoch 0 batch 16 loss: 3.806503786760218 (cur batch: 3.5157599449157715)\nEpoch 0 batch 17 loss: 3.7934765021006265 (cur batch: 3.5720126628875732)\nEpoch 0 batch 18 loss: 3.7730140560551693 (cur batch: 3.4046900272369385)\nEpoch 0 batch 19 loss: 3.7708853244781495 (cur batch: 3.7304394245147705)\nEpoch 0 batch 20 loss: 3.7560569558824812 (cur batch: 3.459489583969116)\nEpoch 0 batch 21 loss: 3.7439207055351953 (cur batch: 3.4890594482421875)\nEpoch 0 batch 22 loss: 3.7339641114939814 (cur batch: 3.5149190425872803)\nEpoch 0 batch 23 loss: 3.7259636720021567 (cur batch: 3.5419535636901855)\nEpoch 0 batch 24 loss: 3.7127906036376954 (cur batch: 3.396636962890625)\nEpoch 0 batch 25 loss: 3.6999371235187235 (cur batch: 3.3786001205444336)\nEpoch 0 batch 26 loss: 3.693672189006099 (cur batch: 3.5307838916778564)\nEpoch 0 batch 27 loss: 3.684742816856929 (cur batch: 3.4436497688293457)\nEpoch 0 batch 28 loss: 3.6736328437410553 (cur batch: 3.362553596496582)\nEpoch 0 batch 29 loss: 3.66295641263326 (cur batch: 3.353339910507202)\nEpoch 0 batch 30 loss: 3.654820126871909 (cur batch: 3.410731554031372)\nEpoch 0 batch 31 loss: 3.6488513126969337 (cur batch: 3.463818073272705)\n\t Validation Loss: 3.3199832439422607\nEpoch 0 batch 32 loss: 3.374659538269043 (cur batch: 3.374659538269043)\nEpoch 0 batch 33 loss: 3.2983814477920532 (cur batch: 3.2221033573150635)\nEpoch 0 batch 34 loss: 3.2971935272216797 (cur batch: 3.2948176860809326)\nEpoch 0 batch 35 loss: 3.3221506476402283 (cur batch: 3.397022008895874)\nEpoch 0 batch 36 loss: 3.321409273147583 (cur batch: 3.318443775177002)\nEpoch 0 batch 37 loss: 3.293040633201599 (cur batch: 3.1511974334716797)\nEpoch 0 batch 38 loss: 3.2891030652182445 (cur batch: 3.2654776573181152)\nEpoch 0 batch 39 loss: 3.2935100495815277 (cur batch: 3.3243589401245117)\nEpoch 0 batch 40 loss: 3.282651080025567 (cur batch: 3.195779323577881)\nEpoch 0 batch 41 loss: 3.2736860036849977 (cur batch: 3.193000316619873)\nEpoch 0 batch 42 loss: 3.2716793797232886 (cur batch: 3.251613140106201)\nEpoch 0 batch 43 loss: 3.271636684735616 (cur batch: 3.271167039871216)\nEpoch 0 batch 44 loss: 3.277385014754075 (cur batch: 3.346364974975586)\nEpoch 0 batch 45 loss: 3.2810708624976024 (cur batch: 3.328986883163452)\nEpoch 0 batch 46 loss: 3.2750103791554768 (cur batch: 3.1901636123657227)\nEpoch 0 batch 47 loss: 3.2694219052791595 (cur batch: 3.1855947971343994)\nEpoch 0 batch 48 loss: 3.269896352992338 (cur batch: 3.2774875164031982)\nEpoch 0 batch 49 loss: 3.266156713167826 (cur batch: 3.202582836151123)\nEpoch 0 batch 50 loss: 3.2610249268381217 (cur batch: 3.1686527729034424)\nEpoch 0 batch 51 loss: 3.2602339029312133 (cur batch: 3.245204448699951)\nEpoch 0 batch 52 loss: 3.2563224065871466 (cur batch: 3.1780924797058105)\nEpoch 0 batch 53 loss: 3.2565251046961006 (cur batch: 3.260781764984131)\nEpoch 0 batch 54 loss: 3.2546537544416343 (cur batch: 3.213484048843384)\nEpoch 0 batch 55 loss: 3.2397726078828177 (cur batch: 2.8975062370300293)\nEpoch 0 batch 56 loss: 3.2370025539398193 (cur batch: 3.1705212593078613)\nEpoch 0 batch 57 loss: 3.235193050824679 (cur batch: 3.189955472946167)\nEpoch 0 batch 58 loss: 3.230925056669447 (cur batch: 3.119957208633423)\nEpoch 0 batch 59 loss: 3.226302112851824 (cur batch: 3.101482629776001)\nEpoch 0 batch 60 loss: 3.226243051989325 (cur batch: 3.2245893478393555)\nEpoch 0 batch 61 loss: 3.223433828353882 (cur batch: 3.1419663429260254)\nEpoch 0 batch 62 loss: 3.2202277260441936 (cur batch: 3.12404465675354)\nEpoch 0 batch 63 loss: 3.217056967318058 (cur batch: 3.1187634468078613)\n\t Validation Loss: 3.165886163711548\nEpoch 0 batch 64 loss: 3.1153955459594727 (cur batch: 3.1153955459594727)\nEpoch 0 batch 65 loss: 3.096858024597168 (cur batch: 3.0783205032348633)\nEpoch 0 batch 66 loss: 3.1143800417582193 (cur batch: 3.1494240760803223)\nEpoch 0 batch 67 loss: 3.0945581197738647 (cur batch: 3.035092353820801)\nEpoch 0 batch 68 loss: 3.102909469604492 (cur batch: 3.136314868927002)\nEpoch 0 batch 69 loss: 3.0935811201731362 (cur batch: 3.0469393730163574)\nEpoch 0 batch 70 loss: 3.066690342766898 (cur batch: 2.9053456783294678)\nEpoch 0 batch 71 loss: 3.058813363313675 (cur batch: 3.0036745071411133)\nEpoch 0 batch 72 loss: 3.0534311135609946 (cur batch: 3.010373115539551)\nEpoch 0 batch 73 loss: 3.063068389892578 (cur batch: 3.149803876876831)\nEpoch 0 batch 74 loss: 3.0563363161954014 (cur batch: 2.989015579223633)\nEpoch 0 batch 75 loss: 3.0702461997667947 (cur batch: 3.223254919052124)\nEpoch 0 batch 76 loss: 3.06408561193026 (cur batch: 2.9901585578918457)\nEpoch 0 batch 77 loss: 3.063644085611616 (cur batch: 3.0579042434692383)\nEpoch 0 batch 78 loss: 3.0559828758239744 (cur batch: 2.948725938796997)\nEpoch 0 batch 79 loss: 3.0628110468387604 (cur batch: 3.165233612060547)\nEpoch 0 batch 80 loss: 3.0571376716389373 (cur batch: 2.9663636684417725)\nEpoch 0 batch 81 loss: 3.0525559849209256 (cur batch: 2.9746673107147217)\nEpoch 0 batch 82 loss: 3.04742253454108 (cur batch: 2.9550204277038574)\nEpoch 0 batch 83 loss: 3.0475075125694273 (cur batch: 3.0491220951080322)\nEpoch 0 batch 84 loss: 3.0413008076804027 (cur batch: 2.9171667098999023)\nEpoch 0 batch 85 loss: 3.0338375026529487 (cur batch: 2.877108097076416)\nEpoch 0 batch 86 loss: 3.031256053758704 (cur batch: 2.974464178085327)\nEpoch 0 batch 87 loss: 3.031617224216461 (cur batch: 3.039924144744873)\nEpoch 0 batch 88 loss: 3.0307729339599607 (cur batch: 3.010509967803955)\nEpoch 0 batch 89 loss: 3.0307368681981015 (cur batch: 3.0298352241516113)\nEpoch 0 batch 90 loss: 3.0239096217685275 (cur batch: 2.8464012145996094)\nEpoch 0 batch 91 loss: 3.0237129671233043 (cur batch: 3.0184032917022705)\nEpoch 1 batch 0 loss: 2.886702060699463 (cur batch: 2.886702060699463)\nEpoch 1 batch 1 loss: 2.964037775993347 (cur batch: 3.0413734912872314)\nEpoch 1 batch 2 loss: 2.969592491785685 (cur batch: 2.9807019233703613)\nEpoch 1 batch 3 loss: 3.0046032071113586 (cur batch: 3.109635353088379)\nEpoch 1 batch 4 loss: 2.9852652072906496 (cur batch: 2.9079132080078125)\nEpoch 1 batch 5 loss: 2.9688597520192466 (cur batch: 2.8868324756622314)\nEpoch 1 batch 6 loss: 2.982409954071045 (cur batch: 3.063711166381836)\nEpoch 1 batch 7 loss: 2.987538754940033 (cur batch: 3.023440361022949)\nEpoch 1 batch 8 loss: 2.9871302445729575 (cur batch: 2.9838621616363525)\nEpoch 1 batch 9 loss: 2.9754356384277343 (cur batch: 2.8701841831207275)\nEpoch 1 batch 10 loss: 2.9695360443808814 (cur batch: 2.9105401039123535)\nEpoch 1 batch 11 loss: 2.975369135538737 (cur batch: 3.0395331382751465)\nEpoch 1 batch 12 loss: 2.9757776993971605 (cur batch: 2.980680465698242)\nEpoch 1 batch 13 loss: 2.9572973251342773 (cur batch: 2.717052459716797)\nEpoch 1 batch 14 loss: 2.9576671441396076 (cur batch: 2.9628446102142334)\nEpoch 1 batch 15 loss: 2.961674764752388 (cur batch: 3.021789073944092)\nEpoch 1 batch 16 loss: 2.9574783549589267 (cur batch: 2.89033579826355)\nEpoch 1 batch 17 loss: 2.9473588466644287 (cur batch: 2.775327205657959)\nEpoch 1 batch 18 loss: 2.9465520005477104 (cur batch: 2.9320287704467773)\nEpoch 1 batch 19 loss: 2.9467758178710937 (cur batch: 2.951028347015381)\nEpoch 1 batch 20 loss: 2.945067212695167 (cur batch: 2.9108951091766357)\nEpoch 1 batch 21 loss: 2.944108627059243 (cur batch: 2.923978328704834)\nEpoch 1 batch 22 loss: 2.9382577875386113 (cur batch: 2.809539318084717)\nEpoch 1 batch 23 loss: 2.9383840560913086 (cur batch: 2.9412882328033447)\nEpoch 1 batch 24 loss: 2.9357960987091065 (cur batch: 2.873685121536255)\nEpoch 1 batch 25 loss: 2.9373570680618286 (cur batch: 2.976381301879883)\nEpoch 1 batch 26 loss: 2.9402655054021767 (cur batch: 3.0158848762512207)\nEpoch 1 batch 27 loss: 2.940861369882311 (cur batch: 2.9569497108459473)\nEpoch 1 batch 28 loss: 2.934288271542253 (cur batch: 2.75024151802063)\nEpoch 1 batch 29 loss: 2.931993865966797 (cur batch: 2.8654561042785645)\nEpoch 1 batch 30 loss: 2.927177413817375 (cur batch: 2.782683849334717)\nEpoch 1 batch 31 loss: 2.9271800220012665 (cur batch: 2.9272608757019043)\n\t Validation Loss: 3.1121559937795005\nEpoch 1 batch 32 loss: 3.0266904830932617 (cur batch: 3.0266904830932617)\nEpoch 1 batch 33 loss: 2.945831775665283 (cur batch: 2.8649730682373047)\nEpoch 1 batch 34 loss: 2.9075777530670166 (cur batch: 2.8310697078704834)\nEpoch 1 batch 35 loss: 2.9052406549453735 (cur batch: 2.8982293605804443)\nEpoch 1 batch 36 loss: 2.905305862426758 (cur batch: 2.905566692352295)\nEpoch 1 batch 37 loss: 2.9172910849253335 (cur batch: 2.977217197418213)\nEpoch 1 batch 38 loss: 2.915483679090227 (cur batch: 2.90463924407959)\nEpoch 1 batch 39 loss: 2.9357244670391083 (cur batch: 3.0774099826812744)\nEpoch 1 batch 40 loss: 2.93234560224745 (cur batch: 2.9053146839141846)\nEpoch 1 batch 41 loss: 2.9174394607543945 (cur batch: 2.7832841873168945)\nEpoch 1 batch 42 loss: 2.905922217802568 (cur batch: 2.7907497882843018)\nEpoch 1 batch 43 loss: 2.8974698384602866 (cur batch: 2.8044936656951904)\nEpoch 1 batch 44 loss: 2.889530346943782 (cur batch: 2.7942564487457275)\nEpoch 1 batch 45 loss: 2.8823531014578685 (cur batch: 2.789048910140991)\nEpoch 1 batch 46 loss: 2.8829127470652263 (cur batch: 2.8907477855682373)\nEpoch 1 batch 47 loss: 2.873428225517273 (cur batch: 2.7311604022979736)\nEpoch 1 batch 48 loss: 2.883560096516329 (cur batch: 3.0456700325012207)\nEpoch 1 batch 49 loss: 2.879454493522644 (cur batch: 2.809659242630005)\nEpoch 1 batch 50 loss: 2.8777722935927543 (cur batch: 2.8474926948547363)\nEpoch 1 batch 51 loss: 2.868861186504364 (cur batch: 2.699550151824951)\nEpoch 1 batch 52 loss: 2.8793423175811768 (cur batch: 3.0889649391174316)\nEpoch 1 batch 53 loss: 2.8816701390526513 (cur batch: 2.9305543899536133)\nEpoch 1 batch 54 loss: 2.879552478375642 (cur batch: 2.8329639434814453)\nEpoch 1 batch 55 loss: 2.8775863349437714 (cur batch: 2.832365036010742)\nEpoch 1 batch 56 loss: 2.87510516166687 (cur batch: 2.8155570030212402)\nEpoch 1 batch 57 loss: 2.872280845275292 (cur batch: 2.80167293548584)\nEpoch 1 batch 58 loss: 2.8737133608924017 (cur batch: 2.910958766937256)\nEpoch 1 batch 59 loss: 2.869340862546648 (cur batch: 2.7512834072113037)\nEpoch 1 batch 60 loss: 2.8680411042838263 (cur batch: 2.8316478729248047)\nEpoch 1 batch 61 loss: 2.864106877644857 (cur batch: 2.750014305114746)\nEpoch 1 batch 62 loss: 2.8600335121154785 (cur batch: 2.737832546234131)\nEpoch 1 batch 63 loss: 2.860954023897648 (cur batch: 2.8894898891448975)\n\t Validation Loss: 3.1259979406992593\nEpoch 1 batch 64 loss: 2.7573986053466797 (cur batch: 2.7573986053466797)\nEpoch 1 batch 65 loss: 2.7926418781280518 (cur batch: 2.827885150909424)\nEpoch 1 batch 66 loss: 2.804105599721273 (cur batch: 2.827033042907715)\nEpoch 1 batch 67 loss: 2.828165054321289 (cur batch: 2.900343418121338)\nEpoch 1 batch 68 loss: 2.8151801586151124 (cur batch: 2.7632405757904053)\nEpoch 1 batch 69 loss: 2.8020596901575723 (cur batch: 2.736457347869873)\nEpoch 1 batch 70 loss: 2.8205370221819197 (cur batch: 2.931401014328003)\nEpoch 1 batch 71 loss: 2.8252580761909485 (cur batch: 2.8583054542541504)\nEpoch 1 batch 72 loss: 2.807859844631619 (cur batch: 2.6686739921569824)\nEpoch 1 batch 73 loss: 2.81198308467865 (cur batch: 2.8490922451019287)\nEpoch 1 batch 74 loss: 2.805632309480147 (cur batch: 2.742124557495117)\nEpoch 1 batch 75 loss: 2.8085229992866516 (cur batch: 2.840320587158203)\nEpoch 1 batch 76 loss: 2.8027419860546408 (cur batch: 2.733369827270508)\nEpoch 1 batch 77 loss: 2.8070148399897983 (cur batch: 2.8625619411468506)\nEpoch 1 batch 78 loss: 2.79470157623291 (cur batch: 2.6223158836364746)\nEpoch 1 batch 79 loss: 2.799842655658722 (cur batch: 2.8769588470458984)\nEpoch 1 batch 80 loss: 2.7971290700575886 (cur batch: 2.753711700439453)\nEpoch 1 batch 81 loss: 2.7924857669406467 (cur batch: 2.7135496139526367)\nEpoch 1 batch 82 loss: 2.7859700604488977 (cur batch: 2.668687343597412)\nEpoch 1 batch 83 loss: 2.7834590554237364 (cur batch: 2.7357499599456787)\nEpoch 1 batch 84 loss: 2.7845121905917214 (cur batch: 2.805574893951416)\nEpoch 1 batch 85 loss: 2.7903615778142754 (cur batch: 2.913198709487915)\nEpoch 1 batch 86 loss: 2.793140971142313 (cur batch: 2.854287624359131)\nEpoch 1 batch 87 loss: 2.7922935287157693 (cur batch: 2.7728023529052734)\nEpoch 1 batch 88 loss: 2.7936788845062255 (cur batch: 2.826927423477173)\nEpoch 1 batch 89 loss: 2.797406187424293 (cur batch: 2.8905887603759766)\nEpoch 1 batch 90 loss: 2.806306565249408 (cur batch: 3.0377163887023926)\nEpoch 1 batch 91 loss: 2.8100203616278514 (cur batch: 2.910292863845825)\nEpoch 2 batch 0 loss: 2.834160804748535 (cur batch: 2.834160804748535)\nEpoch 2 batch 1 loss: 2.866276979446411 (cur batch: 2.898393154144287)\nEpoch 2 batch 2 loss: 2.8134623368581138 (cur batch: 2.7078330516815186)\nEpoch 2 batch 3 loss: 2.7847684025764465 (cur batch: 2.6986865997314453)\nEpoch 2 batch 4 loss: 2.787198305130005 (cur batch: 2.7969179153442383)\nEpoch 2 batch 5 loss: 2.7715614239374795 (cur batch: 2.6933770179748535)\nEpoch 2 batch 6 loss: 2.76483838898795 (cur batch: 2.7245001792907715)\nEpoch 2 batch 7 loss: 2.759984940290451 (cur batch: 2.726010799407959)\nEpoch 2 batch 8 loss: 2.733625676896837 (cur batch: 2.522751569747925)\nEpoch 2 batch 9 loss: 2.7172029972076417 (cur batch: 2.569398880004883)\nEpoch 2 batch 10 loss: 2.7254007296128706 (cur batch: 2.807378053665161)\nEpoch 2 batch 11 loss: 2.7262530525525412 (cur batch: 2.735628604888916)\nEpoch 2 batch 12 loss: 2.7300577897291918 (cur batch: 2.775714635848999)\nEpoch 2 batch 13 loss: 2.731852190835135 (cur batch: 2.7551794052124023)\nEpoch 2 batch 14 loss: 2.736996857325236 (cur batch: 2.8090221881866455)\nEpoch 2 batch 15 loss: 2.742891773581505 (cur batch: 2.831315517425537)\nEpoch 2 batch 16 loss: 2.7450258170857147 (cur batch: 2.779170513153076)\nEpoch 2 batch 17 loss: 2.7517595423592462 (cur batch: 2.8662328720092773)\nEpoch 2 batch 18 loss: 2.758392961401688 (cur batch: 2.8777945041656494)\nEpoch 2 batch 19 loss: 2.762624406814575 (cur batch: 2.843021869659424)\nEpoch 2 batch 20 loss: 2.762586071377709 (cur batch: 2.761819362640381)\nEpoch 2 batch 21 loss: 2.768591057170521 (cur batch: 2.89469575881958)\nEpoch 2 batch 22 loss: 2.7675503544185474 (cur batch: 2.744654893875122)\nEpoch 2 batch 23 loss: 2.7722970147927604 (cur batch: 2.881470203399658)\nEpoch 2 batch 24 loss: 2.770205717086792 (cur batch: 2.7200145721435547)\nEpoch 2 batch 25 loss: 2.769527270243718 (cur batch: 2.75256609916687)\nEpoch 2 batch 26 loss: 2.765935509293168 (cur batch: 2.6725497245788574)\nEpoch 2 batch 27 loss: 2.7622770752225603 (cur batch: 2.663499355316162)\nEpoch 2 batch 28 loss: 2.7589053285532983 (cur batch: 2.664496421813965)\nEpoch 2 batch 29 loss: 2.761940511067708 (cur batch: 2.8499608039855957)\nEpoch 2 batch 30 loss: 2.7561369788262153 (cur batch: 2.582031011581421)\nEpoch 2 batch 31 loss: 2.757573150098324 (cur batch: 2.8020944595336914)\n\t Validation Loss: 3.1704323291778564\nEpoch 2 batch 32 loss: 2.878514051437378 (cur batch: 2.878514051437378)\nEpoch 2 batch 33 loss: 2.792759656906128 (cur batch: 2.707005262374878)\nEpoch 2 batch 34 loss: 2.780752658843994 (cur batch: 2.7567386627197266)\nEpoch 2 batch 35 loss: 2.7820189595222473 (cur batch: 2.785817861557007)\nEpoch 2 batch 36 loss: 2.768724775314331 (cur batch: 2.715548038482666)\nEpoch 2 batch 37 loss: 2.754470705986023 (cur batch: 2.6832003593444824)\nEpoch 2 batch 38 loss: 2.7373925617762973 (cur batch: 2.6349236965179443)\nEpoch 2 batch 39 loss: 2.736281991004944 (cur batch: 2.7285079956054688)\nEpoch 2 batch 40 loss: 2.722933053970337 (cur batch: 2.6161415576934814)\nEpoch 2 batch 41 loss: 2.7263243436813354 (cur batch: 2.7568459510803223)\nEpoch 2 batch 42 loss: 2.7318224906921387 (cur batch: 2.786803960800171)\nEpoch 2 batch 43 loss: 2.7256709734598794 (cur batch: 2.6580042839050293)\nEpoch 2 batch 44 loss: 2.7252639623788686 (cur batch: 2.7203798294067383)\nEpoch 2 batch 45 loss: 2.7285358735493253 (cur batch: 2.771070718765259)\nEpoch 2 batch 46 loss: 2.722833522160848 (cur batch: 2.643000602722168)\nEpoch 2 batch 47 loss: 2.7157194912433624 (cur batch: 2.609009027481079)\nEpoch 2 batch 48 loss: 2.7235298717723175 (cur batch: 2.8484959602355957)\nEpoch 2 batch 49 loss: 2.7185801929897733 (cur batch: 2.6344356536865234)\nEpoch 2 batch 50 loss: 2.7152043141816793 (cur batch: 2.6544384956359863)\nEpoch 2 batch 51 loss: 2.7196970224380492 (cur batch: 2.805058479309082)\nEpoch 2 batch 52 loss: 2.7174142655872164 (cur batch: 2.6717591285705566)\nEpoch 2 batch 53 loss: 2.713934822516008 (cur batch: 2.64086651802063)\nEpoch 2 batch 54 loss: 2.713276479555213 (cur batch: 2.6987929344177246)\nEpoch 2 batch 55 loss: 2.7124193410078683 (cur batch: 2.6927051544189453)\nEpoch 2 batch 56 loss: 2.7089301204681395 (cur batch: 2.6251888275146484)\nEpoch 2 batch 57 loss: 2.70902249446282 (cur batch: 2.711331844329834)\nEpoch 2 batch 58 loss: 2.712704640847665 (cur batch: 2.8084404468536377)\nEpoch 2 batch 59 loss: 2.7106070688792636 (cur batch: 2.653972625732422)\nEpoch 2 batch 60 loss: 2.7122544337963235 (cur batch: 2.758380651473999)\nEpoch 2 batch 61 loss: 2.713369027773539 (cur batch: 2.745692253112793)\nEpoch 2 batch 62 loss: 2.71392656910804 (cur batch: 2.7306528091430664)\nEpoch 2 batch 63 loss: 2.7180295810103416 (cur batch: 2.8452229499816895)\n\t Validation Loss: 3.133197546005249\nEpoch 2 batch 64 loss: 2.6510117053985596 (cur batch: 2.6510117053985596)\nEpoch 2 batch 65 loss: 2.6790963411331177 (cur batch: 2.707180976867676)\nEpoch 2 batch 66 loss: 2.711456616719564 (cur batch: 2.776177167892456)\nEpoch 2 batch 67 loss: 2.7091428637504578 (cur batch: 2.7022016048431396)\nEpoch 2 batch 68 loss: 2.7100295543670656 (cur batch: 2.713576316833496)\nEpoch 2 batch 69 loss: 2.70429003238678 (cur batch: 2.6755924224853516)\nEpoch 2 batch 70 loss: 2.7134718213762556 (cur batch: 2.7685625553131104)\nEpoch 2 batch 71 loss: 2.7119078636169434 (cur batch: 2.700960159301758)\nEpoch 2 batch 72 loss: 2.7089296446906195 (cur batch: 2.6851038932800293)\nEpoch 2 batch 73 loss: 2.71439368724823 (cur batch: 2.7635700702667236)\nEpoch 2 batch 74 loss: 2.7085783048109575 (cur batch: 2.6504244804382324)\nEpoch 2 batch 75 loss: 2.71541166305542 (cur batch: 2.790578603744507)\nEpoch 2 batch 76 loss: 2.707965429012592 (cur batch: 2.6186106204986572)\nEpoch 2 batch 77 loss: 2.7010386160441806 (cur batch: 2.610990047454834)\nEpoch 2 batch 78 loss: 2.7073130130767824 (cur batch: 2.795154571533203)\nEpoch 2 batch 79 loss: 2.7081489264965057 (cur batch: 2.7206876277923584)\nEpoch 2 batch 80 loss: 2.6972992560442757 (cur batch: 2.5237045288085938)\nEpoch 2 batch 81 loss: 2.695028132862515 (cur batch: 2.656419038772583)\nEpoch 2 batch 82 loss: 2.695465451792667 (cur batch: 2.7033371925354004)\nEpoch 2 batch 83 loss: 2.7010461688041687 (cur batch: 2.807079792022705)\nEpoch 2 batch 84 loss: 2.7021517867133733 (cur batch: 2.724264144897461)\nEpoch 2 batch 85 loss: 2.7002964344891636 (cur batch: 2.6613340377807617)\nEpoch 2 batch 86 loss: 2.691966150117957 (cur batch: 2.508699893951416)\nEpoch 2 batch 87 loss: 2.6975379486878714 (cur batch: 2.8256893157958984)\nEpoch 2 batch 88 loss: 2.7017306995391848 (cur batch: 2.802356719970703)\nEpoch 2 batch 89 loss: 2.704499290539668 (cur batch: 2.773714065551758)\nEpoch 2 batch 90 loss: 2.702344205644396 (cur batch: 2.6463119983673096)\nEpoch 2 batch 91 loss: 2.695587771279471 (cur batch: 2.5131640434265137)\nEpoch 3 batch 0 loss: 2.731832981109619 (cur batch: 2.731832981109619)\nEpoch 3 batch 1 loss: 2.7629854679107666 (cur batch: 2.794137954711914)\nEpoch 3 batch 2 loss: 2.7592376867930093 (cur batch: 2.751742124557495)\nEpoch 3 batch 3 loss: 2.7407838106155396 (cur batch: 2.68542218208313)\nEpoch 3 batch 4 loss: 2.7144742965698243 (cur batch: 2.609236240386963)\nEpoch 3 batch 5 loss: 2.6974497636159263 (cur batch: 2.6123270988464355)\nEpoch 3 batch 6 loss: 2.675424780164446 (cur batch: 2.5432748794555664)\nEpoch 3 batch 7 loss: 2.67168191075325 (cur batch: 2.645481824874878)\nEpoch 3 batch 8 loss: 2.6719747914208307 (cur batch: 2.6743178367614746)\nEpoch 3 batch 9 loss: 2.6793540000915526 (cur batch: 2.7457668781280518)\nEpoch 3 batch 10 loss: 2.6734840869903564 (cur batch: 2.6147849559783936)\nEpoch 3 batch 11 loss: 2.678630312283834 (cur batch: 2.735238790512085)\nEpoch 3 batch 12 loss: 2.668414519383357 (cur batch: 2.5458250045776367)\nEpoch 3 batch 13 loss: 2.6656935044697354 (cur batch: 2.6303203105926514)\nEpoch 3 batch 14 loss: 2.6689956188201904 (cur batch: 2.7152252197265625)\nEpoch 3 batch 15 loss: 2.6715721487998962 (cur batch: 2.7102200984954834)\nEpoch 3 batch 16 loss: 2.6765880444470573 (cur batch: 2.7568423748016357)\nEpoch 3 batch 17 loss: 2.67445969581604 (cur batch: 2.638277769088745)\nEpoch 3 batch 18 loss: 2.674114892357274 (cur batch: 2.6679084300994873)\nEpoch 3 batch 19 loss: 2.682843554019928 (cur batch: 2.8486881256103516)\nEpoch 3 batch 20 loss: 2.681094521567935 (cur batch: 2.646113872528076)\nEpoch 3 batch 21 loss: 2.6844392364675347 (cur batch: 2.754678249359131)\nEpoch 3 batch 22 loss: 2.682270516519961 (cur batch: 2.63455867767334)\nEpoch 3 batch 23 loss: 2.6816286742687225 (cur batch: 2.6668663024902344)\nEpoch 3 batch 24 loss: 2.681550130844116 (cur batch: 2.6796650886535645)\nEpoch 3 batch 25 loss: 2.678868688069857 (cur batch: 2.611832618713379)\nEpoch 3 batch 26 loss: 2.675364794554534 (cur batch: 2.584263563156128)\nEpoch 3 batch 27 loss: 2.6682825088500977 (cur batch: 2.4770607948303223)\nEpoch 3 batch 28 loss: 2.664227304787471 (cur batch: 2.5506815910339355)\nEpoch 3 batch 29 loss: 2.6606273571650187 (cur batch: 2.5562288761138916)\nEpoch 3 batch 30 loss: 2.659488924088017 (cur batch: 2.625335931777954)\nEpoch 3 batch 31 loss: 2.661781594157219 (cur batch: 2.7328543663024902)\n\t Validation Loss: 3.0761140982309976\nEpoch 3 batch 32 loss: 2.6103920936584473 (cur batch: 2.6103920936584473)\nEpoch 3 batch 33 loss: 2.633396625518799 (cur batch: 2.6564011573791504)\nEpoch 3 batch 34 loss: 2.6272541681925454 (cur batch: 2.614969253540039)\nEpoch 3 batch 35 loss: 2.6089733839035034 (cur batch: 2.554131031036377)\nEpoch 3 batch 36 loss: 2.6231081962585447 (cur batch: 2.679647445678711)\nEpoch 3 batch 37 loss: 2.634130358695984 (cur batch: 2.6892411708831787)\nEpoch 3 batch 38 loss: 2.6358421870640347 (cur batch: 2.646113157272339)\nEpoch 3 batch 39 loss: 2.624184250831604 (cur batch: 2.54257869720459)\nEpoch 3 batch 40 loss: 2.6317741605970593 (cur batch: 2.692493438720703)\nEpoch 3 batch 41 loss: 2.6339301109313964 (cur batch: 2.6533336639404297)\nEpoch 3 batch 42 loss: 2.628849896517667 (cur batch: 2.578047752380371)\nEpoch 3 batch 43 loss: 2.63960466782252 (cur batch: 2.7579071521759033)\nEpoch 3 batch 44 loss: 2.6451197404127855 (cur batch: 2.7113006114959717)\nEpoch 3 batch 45 loss: 2.6461097512926375 (cur batch: 2.658979892730713)\nEpoch 3 batch 46 loss: 2.638041687011719 (cur batch: 2.5250887870788574)\nEpoch 3 batch 47 loss: 2.637485846877098 (cur batch: 2.629148244857788)\nEpoch 3 batch 48 loss: 2.6328870548921475 (cur batch: 2.5593063831329346)\nEpoch 3 batch 49 loss: 2.629941357506646 (cur batch: 2.579864501953125)\nEpoch 3 batch 50 loss: 2.6270762870186255 (cur batch: 2.575505018234253)\nEpoch 3 batch 51 loss: 2.6306460022926332 (cur batch: 2.6984705924987793)\nEpoch 3 batch 52 loss: 2.6219295660654702 (cur batch: 2.447600841522217)\nEpoch 3 batch 53 loss: 2.6193840612064707 (cur batch: 2.5659284591674805)\nEpoch 3 batch 54 loss: 2.6193014538806416 (cur batch: 2.6174840927124023)\nEpoch 3 batch 55 loss: 2.6187332769234977 (cur batch: 2.6056652069091797)\nEpoch 3 batch 56 loss: 2.6168227005004883 (cur batch: 2.5709688663482666)\nEpoch 3 batch 57 loss: 2.6162685797764706 (cur batch: 2.6024155616760254)\nEpoch 3 batch 58 loss: 2.6145185276314065 (cur batch: 2.569017171859741)\nEpoch 3 batch 59 loss: 2.611966005393437 (cur batch: 2.5430479049682617)\nEpoch 3 batch 60 loss: 2.6157555169072646 (cur batch: 2.7218618392944336)\nEpoch 3 batch 61 loss: 2.6143697023391725 (cur batch: 2.574181079864502)\nEpoch 3 batch 62 loss: 2.617157536168252 (cur batch: 2.7007925510406494)\nEpoch 3 batch 63 loss: 2.614067927002907 (cur batch: 2.5182900428771973)\n\t Validation Loss: 3.099483013153076\nEpoch 3 batch 64 loss: 2.4741621017456055 (cur batch: 2.4741621017456055)\nEpoch 3 batch 65 loss: 2.55288028717041 (cur batch: 2.631598472595215)\nEpoch 3 batch 66 loss: 2.5851407051086426 (cur batch: 2.6496615409851074)\nEpoch 3 batch 67 loss: 2.5874881744384766 (cur batch: 2.5945305824279785)\nEpoch 3 batch 68 loss: 2.657384967803955 (cur batch: 2.936972141265869)\nEpoch 3 batch 69 loss: 2.6399738788604736 (cur batch: 2.5529184341430664)\nEpoch 3 batch 70 loss: 2.6464532784053256 (cur batch: 2.6853296756744385)\nEpoch 3 batch 71 loss: 2.647269457578659 (cur batch: 2.652982711791992)\nEpoch 3 batch 72 loss: 2.642412874433729 (cur batch: 2.603560209274292)\nEpoch 3 batch 73 loss: 2.635894536972046 (cur batch: 2.5772294998168945)\nEpoch 3 batch 74 loss: 2.637401992624456 (cur batch: 2.6524765491485596)\nEpoch 3 batch 75 loss: 2.6429138779640198 (cur batch: 2.7035446166992188)\nEpoch 3 batch 76 loss: 2.649202511860774 (cur batch: 2.724666118621826)\nEpoch 3 batch 77 loss: 2.6415848902293613 (cur batch: 2.542555809020996)\nEpoch 3 batch 78 loss: 2.632765547434489 (cur batch: 2.5092947483062744)\nEpoch 3 batch 79 loss: 2.6319568902254105 (cur batch: 2.6198270320892334)\nEpoch 3 batch 80 loss: 2.636702495462754 (cur batch: 2.712632179260254)\nEpoch 3 batch 81 loss: 2.639084431860182 (cur batch: 2.679577350616455)\nEpoch 3 batch 82 loss: 2.637487348757292 (cur batch: 2.6087398529052734)\nEpoch 3 batch 83 loss: 2.637604629993439 (cur batch: 2.6398329734802246)\nEpoch 3 batch 84 loss: 2.6377573808034263 (cur batch: 2.640812397003174)\nEpoch 3 batch 85 loss: 2.636342406272888 (cur batch: 2.606627941131592)\nEpoch 3 batch 86 loss: 2.6403727013131846 (cur batch: 2.729039192199707)\nEpoch 3 batch 87 loss: 2.6376540760199227 (cur batch: 2.5751256942749023)\nEpoch 3 batch 88 loss: 2.635778474807739 (cur batch: 2.590764045715332)\nEpoch 3 batch 89 loss: 2.6362368968816905 (cur batch: 2.6476974487304688)\nEpoch 3 batch 90 loss: 2.643477713620221 (cur batch: 2.8317389488220215)\nEpoch 3 batch 91 loss: 2.638510320867811 (cur batch: 2.5043907165527344)\nEpoch 4 batch 0 loss: 2.689060688018799 (cur batch: 2.689060688018799)\nEpoch 4 batch 1 loss: 2.690737724304199 (cur batch: 2.6924147605895996)\nEpoch 4 batch 2 loss: 2.6730078061421714 (cur batch: 2.6375479698181152)\nEpoch 4 batch 3 loss: 2.6220377683639526 (cur batch: 2.469127655029297)\nEpoch 4 batch 4 loss: 2.6497805595397947 (cur batch: 2.760751724243164)\nEpoch 4 batch 5 loss: 2.6458816130956015 (cur batch: 2.626386880874634)\nEpoch 4 batch 6 loss: 2.636493308203561 (cur batch: 2.5801634788513184)\nEpoch 4 batch 7 loss: 2.644090622663498 (cur batch: 2.6972718238830566)\nEpoch 4 batch 8 loss: 2.6374671989017062 (cur batch: 2.584479808807373)\nEpoch 4 batch 9 loss: 2.6348926544189455 (cur batch: 2.6117217540740967)\nEpoch 4 batch 10 loss: 2.6491543379696934 (cur batch: 2.791771173477173)\nEpoch 4 batch 11 loss: 2.643903930981954 (cur batch: 2.5861494541168213)\nEpoch 4 batch 12 loss: 2.6461494335761437 (cur batch: 2.673095464706421)\nEpoch 4 batch 13 loss: 2.6507513182503835 (cur batch: 2.710575819015503)\nEpoch 4 batch 14 loss: 2.660680866241455 (cur batch: 2.799694538116455)\nEpoch 4 batch 15 loss: 2.6589024513959885 (cur batch: 2.6322262287139893)\nEpoch 4 batch 16 loss: 2.661976519752951 (cur batch: 2.7111616134643555)\nEpoch 4 batch 17 loss: 2.6569825543297663 (cur batch: 2.57208514213562)\nEpoch 4 batch 18 loss: 2.6550156693709526 (cur batch: 2.6196117401123047)\nEpoch 4 batch 19 loss: 2.65914990901947 (cur batch: 2.7377004623413086)\nEpoch 4 batch 20 loss: 2.662056150890532 (cur batch: 2.7201809883117676)\nEpoch 4 batch 21 loss: 2.657870650291443 (cur batch: 2.5699751377105713)\nEpoch 4 batch 22 loss: 2.657338505205901 (cur batch: 2.6456313133239746)\nEpoch 4 batch 23 loss: 2.648630291223526 (cur batch: 2.4483413696289062)\nEpoch 4 batch 24 loss: 2.643065166473389 (cur batch: 2.5095021724700928)\nEpoch 4 batch 25 loss: 2.639597012446477 (cur batch: 2.5528931617736816)\nEpoch 4 batch 26 loss: 2.6369579279864275 (cur batch: 2.5683417320251465)\nEpoch 4 batch 27 loss: 2.637227381978716 (cur batch: 2.644502639770508)\nEpoch 4 batch 28 loss: 2.632897738752694 (cur batch: 2.5116677284240723)\nEpoch 4 batch 29 loss: 2.6299269994099936 (cur batch: 2.5437755584716797)\nEpoch 4 batch 30 loss: 2.6277590413247385 (cur batch: 2.56272029876709)\nEpoch 4 batch 31 loss: 2.6265242472290993 (cur batch: 2.5882456302642822)\n\t Validation Loss: 3.0250736077626548\nEpoch 4 batch 32 loss: 2.634467124938965 (cur batch: 2.634467124938965)\nEpoch 4 batch 33 loss: 2.6939245462417603 (cur batch: 2.7533819675445557)\nEpoch 4 batch 34 loss: 2.6431408723195395 (cur batch: 2.5415735244750977)\nEpoch 4 batch 35 loss: 2.633974552154541 (cur batch: 2.606475591659546)\nEpoch 4 batch 36 loss: 2.6297162055969237 (cur batch: 2.612682819366455)\nEpoch 4 batch 37 loss: 2.6248912811279297 (cur batch: 2.600766658782959)\nEpoch 4 batch 38 loss: 2.616234677178519 (cur batch: 2.5642950534820557)\nEpoch 4 batch 39 loss: 2.6097861528396606 (cur batch: 2.5646464824676514)\nEpoch 4 batch 40 loss: 2.6138673888312445 (cur batch: 2.646517276763916)\nEpoch 4 batch 41 loss: 2.6084755659103394 (cur batch: 2.5599491596221924)\nEpoch 4 batch 42 loss: 2.611828088760376 (cur batch: 2.645353317260742)\nEpoch 4 batch 43 loss: 2.613205134868622 (cur batch: 2.628352642059326)\nEpoch 4 batch 44 loss: 2.622191520837637 (cur batch: 2.7300281524658203)\nEpoch 4 batch 45 loss: 2.630399703979492 (cur batch: 2.7371060848236084)\nEpoch 4 batch 46 loss: 2.628859361012777 (cur batch: 2.6072945594787598)\nEpoch 4 batch 47 loss: 2.6171887516975403 (cur batch: 2.442129611968994)\nEpoch 4 batch 48 loss: 2.617481035344741 (cur batch: 2.622157573699951)\nEpoch 4 batch 49 loss: 2.628526290257772 (cur batch: 2.816295623779297)\nEpoch 4 batch 50 loss: 2.6259584426879883 (cur batch: 2.5797371864318848)\nEpoch 4 batch 51 loss: 2.6202038884162904 (cur batch: 2.5108673572540283)\nEpoch 4 batch 52 loss: 2.614679041362944 (cur batch: 2.5041821002960205)\nEpoch 4 batch 53 loss: 2.6178570769049903 (cur batch: 2.684595823287964)\nEpoch 4 batch 54 loss: 2.615323781967163 (cur batch: 2.559591293334961)\nEpoch 4 batch 55 loss: 2.624652942021688 (cur batch: 2.839223623275757)\nEpoch 4 batch 56 loss: 2.621626625061035 (cur batch: 2.548995018005371)\nEpoch 4 batch 57 loss: 2.6187707369144144 (cur batch: 2.5473735332489014)\nEpoch 4 batch 58 loss: 2.6180595733501293 (cur batch: 2.599569320678711)\nEpoch 4 batch 59 loss: 2.6156703318868364 (cur batch: 2.5511608123779297)\nEpoch 4 batch 60 loss: 2.6115548610687256 (cur batch: 2.496321678161621)\nEpoch 4 batch 61 loss: 2.6093159755071005 (cur batch: 2.5443882942199707)\nEpoch 4 batch 62 loss: 2.606392983467348 (cur batch: 2.5187032222747803)\nEpoch 4 batch 63 loss: 2.6085748448967934 (cur batch: 2.6762125492095947)\n\t Validation Loss: 3.1604472001393638\nEpoch 4 batch 64 loss: 2.5269980430603027 (cur batch: 2.5269980430603027)\nEpoch 4 batch 65 loss: 2.60083270072937 (cur batch: 2.6746673583984375)\nEpoch 4 batch 66 loss: 2.6184611320495605 (cur batch: 2.6537179946899414)\nEpoch 4 batch 67 loss: 2.6369768381118774 (cur batch: 2.692523956298828)\nEpoch 4 batch 68 loss: 2.6329577445983885 (cur batch: 2.6168813705444336)\nEpoch 4 batch 69 loss: 2.627790093421936 (cur batch: 2.601951837539673)\nEpoch 4 batch 70 loss: 2.607663529259818 (cur batch: 2.4869041442871094)\nEpoch 4 batch 71 loss: 2.6072517931461334 (cur batch: 2.604369640350342)\nEpoch 4 batch 72 loss: 2.6101922723982067 (cur batch: 2.633716106414795)\nEpoch 4 batch 73 loss: 2.6203312397003176 (cur batch: 2.7115819454193115)\nEpoch 4 batch 74 loss: 2.6034375754269687 (cur batch: 2.4345009326934814)\nEpoch 4 batch 75 loss: 2.607580045859019 (cur batch: 2.6531472206115723)\nEpoch 4 batch 76 loss: 2.6098430706904483 (cur batch: 2.6369993686676025)\nEpoch 4 batch 77 loss: 2.6040031909942627 (cur batch: 2.5280847549438477)\nEpoch 4 batch 78 loss: 2.605654732386271 (cur batch: 2.6287763118743896)\nEpoch 4 batch 79 loss: 2.59888131916523 (cur batch: 2.4972801208496094)\nEpoch 4 batch 80 loss: 2.598439230638392 (cur batch: 2.5913658142089844)\nEpoch 4 batch 81 loss: 2.6013945870929294 (cur batch: 2.6516356468200684)\nEpoch 4 batch 82 loss: 2.6001116602044356 (cur batch: 2.577018976211548)\nEpoch 4 batch 83 loss: 2.592126226425171 (cur batch: 2.4404029846191406)\nEpoch 4 batch 84 loss: 2.596948805309477 (cur batch: 2.6934003829956055)\nEpoch 4 batch 85 loss: 2.5948139537464487 (cur batch: 2.5499820709228516)\nEpoch 4 batch 86 loss: 2.589065582855888 (cur batch: 2.46260142326355)\nEpoch 4 batch 87 loss: 2.5925746262073517 (cur batch: 2.6732826232910156)\nEpoch 4 batch 88 loss: 2.5905820751190185 (cur batch: 2.5427608489990234)\nEpoch 4 batch 89 loss: 2.5897671626164365 (cur batch: 2.56939435005188)\nEpoch 4 batch 90 loss: 2.5920881871823913 (cur batch: 2.652434825897217)\nEpoch 4 batch 91 loss: 2.58928268296378 (cur batch: 2.5135340690612793)\nEpoch 5 batch 0 loss: 2.6861462593078613 (cur batch: 2.6861462593078613)\nEpoch 5 batch 1 loss: 2.6254372596740723 (cur batch: 2.564728260040283)\nEpoch 5 batch 2 loss: 2.5579453309377036 (cur batch: 2.422961473464966)\nEpoch 5 batch 3 loss: 2.5405885577201843 (cur batch: 2.488518238067627)\nEpoch 5 batch 4 loss: 2.558405876159668 (cur batch: 2.6296751499176025)\nEpoch 5 batch 5 loss: 2.5888383785883584 (cur batch: 2.7410008907318115)\nEpoch 5 batch 6 loss: 2.584195579801287 (cur batch: 2.5563387870788574)\nEpoch 5 batch 7 loss: 2.5954078435897827 (cur batch: 2.673893690109253)\nEpoch 5 batch 8 loss: 2.5908791753980847 (cur batch: 2.554649829864502)\nEpoch 5 batch 9 loss: 2.5950438499450685 (cur batch: 2.63252592086792)\nEpoch 5 batch 10 loss: 2.5911466858603736 (cur batch: 2.5521750450134277)\nEpoch 5 batch 11 loss: 2.5906917452812195 (cur batch: 2.5856873989105225)\nEpoch 5 batch 12 loss: 2.5834582585554857 (cur batch: 2.4966564178466797)\nEpoch 5 batch 13 loss: 2.574894274984087 (cur batch: 2.463562488555908)\nEpoch 5 batch 14 loss: 2.5832439263661704 (cur batch: 2.700139045715332)\nEpoch 5 batch 15 loss: 2.5896078795194626 (cur batch: 2.6850671768188477)\nEpoch 5 batch 16 loss: 2.5941220031065098 (cur batch: 2.6663479804992676)\nEpoch 5 batch 17 loss: 2.60002428955502 (cur batch: 2.7003631591796875)\nEpoch 5 batch 18 loss: 2.6014644848672965 (cur batch: 2.6273880004882812)\nEpoch 5 batch 19 loss: 2.600957918167114 (cur batch: 2.5913331508636475)\nEpoch 5 batch 20 loss: 2.5929338137308755 (cur batch: 2.4324517250061035)\nEpoch 5 batch 21 loss: 2.5946289626034824 (cur batch: 2.6302270889282227)\nEpoch 5 batch 22 loss: 2.5926321693088696 (cur batch: 2.5487027168273926)\nEpoch 5 batch 23 loss: 2.595977157354355 (cur batch: 2.6729118824005127)\nEpoch 5 batch 24 loss: 2.6002755641937254 (cur batch: 2.703437328338623)\nEpoch 5 batch 25 loss: 2.6011101649357724 (cur batch: 2.6219751834869385)\nEpoch 5 batch 26 loss: 2.6003427152280456 (cur batch: 2.5803890228271484)\nEpoch 5 batch 27 loss: 2.600698104926518 (cur batch: 2.6102936267852783)\nEpoch 5 batch 28 loss: 2.59667209099079 (cur batch: 2.4839437007904053)\nEpoch 5 batch 29 loss: 2.595247157414754 (cur batch: 2.553924083709717)\nEpoch 5 batch 30 loss: 2.5959062960840042 (cur batch: 2.615680456161499)\nEpoch 5 batch 31 loss: 2.593201644718647 (cur batch: 2.509357452392578)\n\t Validation Loss: 3.0977629820505777\nEpoch 5 batch 32 loss: 2.5520689487457275 (cur batch: 2.5520689487457275)\nEpoch 5 batch 33 loss: 2.5165421962738037 (cur batch: 2.48101544380188)\nEpoch 5 batch 34 loss: 2.5073272387186685 (cur batch: 2.4888973236083984)\nEpoch 5 batch 35 loss: 2.518250584602356 (cur batch: 2.551020622253418)\nEpoch 5 batch 36 loss: 2.492415428161621 (cur batch: 2.3890748023986816)\nEpoch 5 batch 37 loss: 2.522883892059326 (cur batch: 2.6752262115478516)\nEpoch 5 batch 38 loss: 2.509352684020996 (cur batch: 2.4281654357910156)\nEpoch 5 batch 39 loss: 2.5218859016895294 (cur batch: 2.6096184253692627)\nEpoch 5 batch 40 loss: 2.5264712704552545 (cur batch: 2.5631542205810547)\nEpoch 5 batch 41 loss: 2.552633261680603 (cur batch: 2.7880911827087402)\nEpoch 5 batch 42 loss: 2.5485280860554087 (cur batch: 2.507476329803467)\nEpoch 5 batch 43 loss: 2.5479560494422913 (cur batch: 2.541663646697998)\nEpoch 5 batch 44 loss: 2.5597180953392615 (cur batch: 2.7008626461029053)\nEpoch 5 batch 45 loss: 2.5625578675951277 (cur batch: 2.5994749069213867)\nEpoch 5 batch 46 loss: 2.559518845876058 (cur batch: 2.516972541809082)\nEpoch 5 batch 47 loss: 2.563624754548073 (cur batch: 2.625213384628296)\nEpoch 5 batch 48 loss: 2.557507935692282 (cur batch: 2.459638833999634)\nEpoch 5 batch 49 loss: 2.5584890047709146 (cur batch: 2.575167179107666)\nEpoch 5 batch 50 loss: 2.564340767107512 (cur batch: 2.6696724891662598)\nEpoch 5 batch 51 loss: 2.562694549560547 (cur batch: 2.531416416168213)\nEpoch 5 batch 52 loss: 2.5607765629178 (cur batch: 2.522416830062866)\nEpoch 5 batch 53 loss: 2.5528997724706475 (cur batch: 2.3874871730804443)\nEpoch 5 batch 54 loss: 2.5532220446545146 (cur batch: 2.560312032699585)\nEpoch 5 batch 55 loss: 2.5598377883434296 (cur batch: 2.7119998931884766)\nEpoch 5 batch 56 loss: 2.561988754272461 (cur batch: 2.613611936569214)\nEpoch 5 batch 57 loss: 2.5560814417325535 (cur batch: 2.4083986282348633)\nEpoch 5 batch 58 loss: 2.55643637975057 (cur batch: 2.565664768218994)\nEpoch 5 batch 59 loss: 2.554483345576695 (cur batch: 2.50175142288208)\nEpoch 5 batch 60 loss: 2.556735342946546 (cur batch: 2.619791269302368)\nEpoch 5 batch 61 loss: 2.5525941451390586 (cur batch: 2.432499408721924)\nEpoch 5 batch 62 loss: 2.5511850157091693 (cur batch: 2.5089111328125)\nEpoch 5 batch 63 loss: 2.5554837360978127 (cur batch: 2.688744068145752)\n\t Validation Loss: 3.1003328959147134\nEpoch 5 batch 64 loss: 2.4794797897338867 (cur batch: 2.4794797897338867)\nEpoch 5 batch 65 loss: 2.512363314628601 (cur batch: 2.5452468395233154)\nEpoch 5 batch 66 loss: 2.58508038520813 (cur batch: 2.7305145263671875)\nEpoch 5 batch 67 loss: 2.5787468552589417 (cur batch: 2.559746265411377)\nEpoch 5 batch 68 loss: 2.569109582901001 (cur batch: 2.5305604934692383)\nEpoch 5 batch 69 loss: 2.5710402727127075 (cur batch: 2.5806937217712402)\nEpoch 5 batch 70 loss: 2.5555947508130754 (cur batch: 2.462921619415283)\nEpoch 5 batch 71 loss: 2.5603665709495544 (cur batch: 2.5937693119049072)\nEpoch 5 batch 72 loss: 2.5546125306023493 (cur batch: 2.508580207824707)\nEpoch 5 batch 73 loss: 2.550404357910156 (cur batch: 2.51253080368042)\nEpoch 5 batch 74 loss: 2.555377028205178 (cur batch: 2.6051037311553955)\nEpoch 5 batch 75 loss: 2.5537598927815757 (cur batch: 2.5359714031219482)\nEpoch 5 batch 76 loss: 2.539544398968036 (cur batch: 2.3689584732055664)\nEpoch 5 batch 77 loss: 2.533319813864572 (cur batch: 2.4524002075195312)\nEpoch 5 batch 78 loss: 2.537302509943644 (cur batch: 2.593060255050659)\nEpoch 5 batch 79 loss: 2.5410730987787247 (cur batch: 2.5976319313049316)\nEpoch 5 batch 80 loss: 2.5432494948892033 (cur batch: 2.5780718326568604)\nEpoch 5 batch 81 loss: 2.5487228102154202 (cur batch: 2.6417691707611084)\nEpoch 5 batch 82 loss: 2.5537887246985185 (cur batch: 2.644975185394287)\nEpoch 5 batch 83 loss: 2.5511401772499083 (cur batch: 2.5008177757263184)\nEpoch 5 batch 84 loss: 2.549437579654512 (cur batch: 2.515385627746582)\nEpoch 5 batch 85 loss: 2.5458310409025713 (cur batch: 2.4700937271118164)\nEpoch 5 batch 86 loss: 2.549799701441889 (cur batch: 2.6371102333068848)\nEpoch 5 batch 87 loss: 2.5495886405309043 (cur batch: 2.544734239578247)\nEpoch 5 batch 88 loss: 2.5447573280334472 (cur batch: 2.4288058280944824)\nEpoch 5 batch 89 loss: 2.5403953332167406 (cur batch: 2.4313454627990723)\nEpoch 5 batch 90 loss: 2.5412465731302896 (cur batch: 2.5633788108825684)\nEpoch 5 batch 91 loss: 2.544764365468706 (cur batch: 2.639744758605957)\nEpoch 6 batch 0 loss: 2.6573219299316406 (cur batch: 2.6573219299316406)\nEpoch 6 batch 1 loss: 2.5602874755859375 (cur batch: 2.4632530212402344)\nEpoch 6 batch 2 loss: 2.5401031176249185 (cur batch: 2.499734401702881)\nEpoch 6 batch 3 loss: 2.505620777606964 (cur batch: 2.4021737575531006)\nEpoch 6 batch 4 loss: 2.531975507736206 (cur batch: 2.637394428253174)\nEpoch 6 batch 5 loss: 2.5324145952860513 (cur batch: 2.5346100330352783)\nEpoch 6 batch 6 loss: 2.5207718440464566 (cur batch: 2.4509153366088867)\nEpoch 6 batch 7 loss: 2.518834203481674 (cur batch: 2.5052707195281982)\nEpoch 6 batch 8 loss: 2.50429023636712 (cur batch: 2.3879384994506836)\nEpoch 6 batch 9 loss: 2.508055090904236 (cur batch: 2.5419387817382812)\nEpoch 6 batch 10 loss: 2.5072778571735728 (cur batch: 2.4995055198669434)\nEpoch 6 batch 11 loss: 2.51466965675354 (cur batch: 2.5959794521331787)\nEpoch 6 batch 12 loss: 2.5146602300497203 (cur batch: 2.514547109603882)\nEpoch 6 batch 13 loss: 2.5208310910633633 (cur batch: 2.6010522842407227)\nEpoch 6 batch 14 loss: 2.519813903172811 (cur batch: 2.505573272705078)\nEpoch 6 batch 15 loss: 2.5190934389829636 (cur batch: 2.508286476135254)\nEpoch 6 batch 16 loss: 2.5172801578746125 (cur batch: 2.488267660140991)\nEpoch 6 batch 17 loss: 2.5260366996129355 (cur batch: 2.6748979091644287)\nEpoch 6 batch 18 loss: 2.529579024565847 (cur batch: 2.5933408737182617)\nEpoch 6 batch 19 loss: 2.5310171008110047 (cur batch: 2.558340549468994)\nEpoch 6 batch 20 loss: 2.5382451443445113 (cur batch: 2.6828060150146484)\nEpoch 6 batch 21 loss: 2.540546428073536 (cur batch: 2.5888733863830566)\nEpoch 6 batch 22 loss: 2.5384926173997964 (cur batch: 2.4933087825775146)\nEpoch 6 batch 23 loss: 2.53648449977239 (cur batch: 2.490297794342041)\nEpoch 6 batch 24 loss: 2.5385302543640136 (cur batch: 2.5876283645629883)\nEpoch 6 batch 25 loss: 2.539776453605065 (cur batch: 2.5709314346313477)\nEpoch 6 batch 26 loss: 2.5361172093285456 (cur batch: 2.440976858139038)\nEpoch 6 batch 27 loss: 2.542143796171461 (cur batch: 2.704861640930176)\nEpoch 6 batch 28 loss: 2.5408665311747582 (cur batch: 2.50510311126709)\nEpoch 6 batch 29 loss: 2.543500820795695 (cur batch: 2.6198952198028564)\nEpoch 6 batch 30 loss: 2.5419357207513626 (cur batch: 2.4949827194213867)\nEpoch 6 batch 31 loss: 2.5406078696250916 (cur batch: 2.4994444847106934)\n\t Validation Loss: 3.1291943391164145\nEpoch 6 batch 32 loss: 2.5416173934936523 (cur batch: 2.5416173934936523)\nEpoch 6 batch 33 loss: 2.569339394569397 (cur batch: 2.5970613956451416)\nEpoch 6 batch 34 loss: 2.5661864280700684 (cur batch: 2.559880495071411)\nEpoch 6 batch 35 loss: 2.542322516441345 (cur batch: 2.470730781555176)\nEpoch 6 batch 36 loss: 2.559944915771484 (cur batch: 2.630434513092041)\nEpoch 6 batch 37 loss: 2.562587777773539 (cur batch: 2.5758020877838135)\nEpoch 6 batch 38 loss: 2.557443448475429 (cur batch: 2.5265774726867676)\nEpoch 6 batch 39 loss: 2.5525083243846893 (cur batch: 2.5179624557495117)\nEpoch 6 batch 40 loss: 2.5529217984941273 (cur batch: 2.556229591369629)\nEpoch 6 batch 41 loss: 2.556198239326477 (cur batch: 2.585686206817627)\nEpoch 6 batch 42 loss: 2.540300737727772 (cur batch: 2.3813257217407227)\nEpoch 6 batch 43 loss: 2.551870803038279 (cur batch: 2.6791415214538574)\nEpoch 6 batch 44 loss: 2.5504716543050914 (cur batch: 2.533681869506836)\nEpoch 6 batch 45 loss: 2.5481854336602345 (cur batch: 2.5184645652770996)\nEpoch 6 batch 46 loss: 2.553860553105672 (cur batch: 2.633312225341797)\nEpoch 6 batch 47 loss: 2.548631802201271 (cur batch: 2.470200538635254)\nEpoch 6 batch 48 loss: 2.5495095393236946 (cur batch: 2.5635533332824707)\nEpoch 6 batch 49 loss: 2.551574852731493 (cur batch: 2.5866851806640625)\nEpoch 6 batch 50 loss: 2.558854818344116 (cur batch: 2.689894199371338)\nEpoch 6 batch 51 loss: 2.557549571990967 (cur batch: 2.532749891281128)\nEpoch 6 batch 52 loss: 2.5581372578938804 (cur batch: 2.5698909759521484)\nEpoch 6 batch 53 loss: 2.5593240694566206 (cur batch: 2.58424711227417)\nEpoch 6 batch 54 loss: 2.557370372440504 (cur batch: 2.5143890380859375)\nEpoch 6 batch 55 loss: 2.5500363310178122 (cur batch: 2.3813533782958984)\nEpoch 6 batch 56 loss: 2.552649459838867 (cur batch: 2.6153645515441895)\nEpoch 6 batch 57 loss: 2.5528086240474996 (cur batch: 2.5567877292633057)\nEpoch 6 batch 58 loss: 2.5516617121519864 (cur batch: 2.5218420028686523)\nEpoch 6 batch 59 loss: 2.5479024989264354 (cur batch: 2.446403741836548)\nEpoch 6 batch 60 loss: 2.551088711311077 (cur batch: 2.6403026580810547)\nEpoch 6 batch 61 loss: 2.54706015586853 (cur batch: 2.430232048034668)\nEpoch 6 batch 62 loss: 2.5462225175672963 (cur batch: 2.5210933685302734)\nEpoch 6 batch 63 loss: 2.548175171017647 (cur batch: 2.6087074279785156)\n\t Validation Loss: 3.0102402369181314\nEpoch 6 batch 64 loss: 2.379138946533203 (cur batch: 2.379138946533203)\nEpoch 6 batch 65 loss: 2.415453314781189 (cur batch: 2.451767683029175)\nEpoch 6 batch 66 loss: 2.452427625656128 (cur batch: 2.526376247406006)\nEpoch 6 batch 67 loss: 2.46809846162796 (cur batch: 2.515110969543457)\nEpoch 6 batch 68 loss: 2.4915693759918214 (cur batch: 2.5854530334472656)\nEpoch 6 batch 69 loss: 2.4936170180638633 (cur batch: 2.5038552284240723)\nEpoch 6 batch 70 loss: 2.4967818600790843 (cur batch: 2.51577091217041)\nEpoch 6 batch 71 loss: 2.496538907289505 (cur batch: 2.494838237762451)\nEpoch 6 batch 72 loss: 2.500115050209893 (cur batch: 2.528724193572998)\nEpoch 6 batch 73 loss: 2.4873422384262085 (cur batch: 2.372386932373047)\nEpoch 6 batch 74 loss: 2.4831501570614902 (cur batch: 2.4412293434143066)\nEpoch 6 batch 75 loss: 2.484678089618683 (cur batch: 2.5014853477478027)\nEpoch 6 batch 76 loss: 2.4911062717437744 (cur batch: 2.568244457244873)\nEpoch 6 batch 77 loss: 2.4965736865997314 (cur batch: 2.567650079727173)\nEpoch 6 batch 78 loss: 2.4934004465738933 (cur batch: 2.448975086212158)\nEpoch 6 batch 79 loss: 2.493463635444641 (cur batch: 2.4944114685058594)\nEpoch 6 batch 80 loss: 2.4948279436896827 (cur batch: 2.5166568756103516)\nEpoch 6 batch 81 loss: 2.499949163860745 (cur batch: 2.587009906768799)\nEpoch 6 batch 82 loss: 2.498920390480443 (cur batch: 2.4804024696350098)\nEpoch 6 batch 83 loss: 2.4936543107032776 (cur batch: 2.393598794937134)\nEpoch 6 batch 84 loss: 2.495269775390625 (cur batch: 2.5275790691375732)\nEpoch 6 batch 85 loss: 2.5000577081333506 (cur batch: 2.600604295730591)\nEpoch 6 batch 86 loss: 2.498618053353351 (cur batch: 2.4669456481933594)\nEpoch 6 batch 87 loss: 2.4980368117491403 (cur batch: 2.484668254852295)\nEpoch 6 batch 88 loss: 2.496074571609497 (cur batch: 2.4489808082580566)\nEpoch 6 batch 89 loss: 2.5019204708246083 (cur batch: 2.6480679512023926)\nEpoch 6 batch 90 loss: 2.503314910111604 (cur batch: 2.5395703315734863)\nEpoch 6 batch 91 loss: 2.5035875609942844 (cur batch: 2.51094913482666)\nEpoch 7 batch 0 loss: 2.4864912033081055 (cur batch: 2.4864912033081055)\nEpoch 7 batch 1 loss: 2.5450499057769775 (cur batch: 2.6036086082458496)\nEpoch 7 batch 2 loss: 2.606570561726888 (cur batch: 2.729611873626709)\nEpoch 7 batch 3 loss: 2.5441919565200806 (cur batch: 2.357056140899658)\nEpoch 7 batch 4 loss: 2.5401597023010254 (cur batch: 2.5240306854248047)\nEpoch 7 batch 5 loss: 2.5489793618520102 (cur batch: 2.5930776596069336)\nEpoch 7 batch 6 loss: 2.5190573079245433 (cur batch: 2.339524984359741)\nEpoch 7 batch 7 loss: 2.5262310802936554 (cur batch: 2.5764474868774414)\nEpoch 7 batch 8 loss: 2.512060615751478 (cur batch: 2.3986968994140625)\nEpoch 7 batch 9 loss: 2.523017978668213 (cur batch: 2.6216342449188232)\nEpoch 7 batch 10 loss: 2.518890532580289 (cur batch: 2.47761607170105)\nEpoch 7 batch 11 loss: 2.5211769143740335 (cur batch: 2.5463271141052246)\nEpoch 7 batch 12 loss: 2.507669503872211 (cur batch: 2.345580577850342)\nEpoch 7 batch 13 loss: 2.5163889782769338 (cur batch: 2.62974214553833)\nEpoch 7 batch 14 loss: 2.527586507797241 (cur batch: 2.684351921081543)\nEpoch 7 batch 15 loss: 2.531126946210861 (cur batch: 2.584233522415161)\nEpoch 7 batch 16 loss: 2.541588446673225 (cur batch: 2.708972454071045)\nEpoch 7 batch 17 loss: 2.540085554122925 (cur batch: 2.5145363807678223)\nEpoch 7 batch 18 loss: 2.5447910835868432 (cur batch: 2.629490613937378)\nEpoch 7 batch 19 loss: 2.5435850381851197 (cur batch: 2.520670175552368)\nEpoch 7 batch 20 loss: 2.5421882356916154 (cur batch: 2.514252185821533)\nEpoch 7 batch 21 loss: 2.5377918048338457 (cur batch: 2.4454667568206787)\nEpoch 7 batch 22 loss: 2.5427629947662354 (cur batch: 2.6521291732788086)\nEpoch 7 batch 23 loss: 2.54621414343516 (cur batch: 2.6255905628204346)\nEpoch 7 batch 24 loss: 2.547292432785034 (cur batch: 2.573171377182007)\nEpoch 7 batch 25 loss: 2.556268361898569 (cur batch: 2.7806665897369385)\nEpoch 7 batch 26 loss: 2.563074791872943 (cur batch: 2.740041971206665)\nEpoch 7 batch 27 loss: 2.5591087511607578 (cur batch: 2.4520256519317627)\nEpoch 7 batch 28 loss: 2.551005034611143 (cur batch: 2.324100971221924)\nEpoch 7 batch 29 loss: 2.5505785942077637 (cur batch: 2.5382118225097656)\nEpoch 7 batch 30 loss: 2.550684159801852 (cur batch: 2.5538511276245117)\nEpoch 7 batch 31 loss: 2.5470895022153854 (cur batch: 2.435655117034912)\n\t Validation Loss: 3.2496777375539145\nEpoch 7 batch 32 loss: 2.5391628742218018 (cur batch: 2.5391628742218018)\nEpoch 7 batch 33 loss: 2.5121614933013916 (cur batch: 2.4851601123809814)\nEpoch 7 batch 34 loss: 2.545896291732788 (cur batch: 2.613365888595581)\nEpoch 7 batch 35 loss: 2.555324614048004 (cur batch: 2.5836095809936523)\nEpoch 7 batch 36 loss: 2.562170171737671 (cur batch: 2.589552402496338)\nEpoch 7 batch 37 loss: 2.5539677143096924 (cur batch: 2.5129554271698)\nEpoch 7 batch 38 loss: 2.5480751650674 (cur batch: 2.5127198696136475)\nEpoch 7 batch 39 loss: 2.56975719332695 (cur batch: 2.721531391143799)\nEpoch 7 batch 40 loss: 2.5890554587046304 (cur batch: 2.743441581726074)\nEpoch 7 batch 41 loss: 2.5868996381759644 (cur batch: 2.5674972534179688)\nEpoch 7 batch 42 loss: 2.5837378718636255 (cur batch: 2.5521202087402344)\nEpoch 7 batch 43 loss: 2.5808597207069397 (cur batch: 2.5492000579833984)\nEpoch 7 batch 44 loss: 2.57719008739178 (cur batch: 2.5331544876098633)\nEpoch 7 batch 45 loss: 2.565008044242859 (cur batch: 2.4066414833068848)\nEpoch 7 batch 46 loss: 2.5661588509877524 (cur batch: 2.5822701454162598)\nEpoch 7 batch 47 loss: 2.5686635822057724 (cur batch: 2.606234550476074)\nEpoch 7 batch 48 loss: 2.5698693920584286 (cur batch: 2.5891623497009277)\nEpoch 7 batch 49 loss: 2.563473926650153 (cur batch: 2.4547510147094727)\nEpoch 7 batch 50 loss: 2.5675099021510075 (cur batch: 2.640157461166382)\nEpoch 7 batch 51 loss: 2.5633189916610717 (cur batch: 2.483691692352295)\nEpoch 7 batch 52 loss: 2.567552770887102 (cur batch: 2.652228355407715)\nEpoch 7 batch 53 loss: 2.5663872198625044 (cur batch: 2.5419106483459473)\nEpoch 7 batch 54 loss: 2.566080839737602 (cur batch: 2.559340476989746)\nEpoch 7 batch 55 loss: 2.568032890558243 (cur batch: 2.6129300594329834)\nEpoch 7 batch 56 loss: 2.5673390197753907 (cur batch: 2.5506861209869385)\nEpoch 7 batch 57 loss: 2.5654448729294996 (cur batch: 2.5180912017822266)\nEpoch 7 batch 58 loss: 2.563747105775056 (cur batch: 2.5196051597595215)\nEpoch 7 batch 59 loss: 2.559773870876857 (cur batch: 2.4524965286254883)\nEpoch 7 batch 60 loss: 2.5626240187677842 (cur batch: 2.642428159713745)\nEpoch 7 batch 61 loss: 2.561345108350118 (cur batch: 2.524256706237793)\nEpoch 7 batch 62 loss: 2.5563317114307034 (cur batch: 2.4059298038482666)\nEpoch 7 batch 63 loss: 2.5537579506635666 (cur batch: 2.473971366882324)\n\t Validation Loss: 3.05929962793986\nEpoch 7 batch 64 loss: 2.522226095199585 (cur batch: 2.522226095199585)\nEpoch 7 batch 65 loss: 2.4647738933563232 (cur batch: 2.4073216915130615)\nEpoch 7 batch 66 loss: 2.4972235361735025 (cur batch: 2.5621228218078613)\nEpoch 7 batch 67 loss: 2.507241666316986 (cur batch: 2.5372960567474365)\nEpoch 7 batch 68 loss: 2.538096714019775 (cur batch: 2.6615169048309326)\nEpoch 7 batch 69 loss: 2.5244463284810386 (cur batch: 2.4561944007873535)\nEpoch 7 batch 70 loss: 2.5123356751033237 (cur batch: 2.439671754837036)\nEpoch 7 batch 71 loss: 2.51516056060791 (cur batch: 2.5349347591400146)\nEpoch 7 batch 72 loss: 2.5032840834723578 (cur batch: 2.4082722663879395)\nEpoch 7 batch 73 loss: 2.5040284156799317 (cur batch: 2.5107274055480957)\nEpoch 7 batch 74 loss: 2.4844194759022105 (cur batch: 2.288330078125)\nEpoch 7 batch 75 loss: 2.4883625507354736 (cur batch: 2.531736373901367)\nEpoch 7 batch 76 loss: 2.4848259412325344 (cur batch: 2.4423866271972656)\nEpoch 7 batch 77 loss: 2.4859682662146434 (cur batch: 2.5008184909820557)\nEpoch 7 batch 78 loss: 2.487865908940633 (cur batch: 2.514432907104492)\nEpoch 7 batch 79 loss: 2.4895990937948227 (cur batch: 2.515596866607666)\nEpoch 7 batch 80 loss: 2.484543702181648 (cur batch: 2.4036574363708496)\nEpoch 7 batch 81 loss: 2.48306765821245 (cur batch: 2.457974910736084)\nEpoch 7 batch 82 loss: 2.485587057314421 (cur batch: 2.5309362411499023)\nEpoch 7 batch 83 loss: 2.484951984882355 (cur batch: 2.4728856086730957)\nEpoch 7 batch 84 loss: 2.48891350201198 (cur batch: 2.568143844604492)\nEpoch 7 batch 85 loss: 2.48768150806427 (cur batch: 2.4618096351623535)\nEpoch 7 batch 86 loss: 2.482746798059215 (cur batch: 2.374183177947998)\nEpoch 7 batch 87 loss: 2.4857417742411294 (cur batch: 2.554626226425171)\nEpoch 7 batch 88 loss: 2.48572172164917 (cur batch: 2.4852404594421387)\nEpoch 7 batch 89 loss: 2.4956368116232066 (cur batch: 2.743514060974121)\nEpoch 7 batch 90 loss: 2.4950792966065585 (cur batch: 2.480583906173706)\nEpoch 7 batch 91 loss: 2.5000022990362987 (cur batch: 2.6329233646392822)\nEpoch 8 batch 0 loss: 2.6577916145324707 (cur batch: 2.6577916145324707)\nEpoch 8 batch 1 loss: 2.5868924856185913 (cur batch: 2.515993356704712)\nEpoch 8 batch 2 loss: 2.5546886920928955 (cur batch: 2.490281105041504)\nEpoch 8 batch 3 loss: 2.550767660140991 (cur batch: 2.5390045642852783)\nEpoch 8 batch 4 loss: 2.539013147354126 (cur batch: 2.491995096206665)\nEpoch 8 batch 5 loss: 2.523867050806681 (cur batch: 2.448136568069458)\nEpoch 8 batch 6 loss: 2.524965967450823 (cur batch: 2.531559467315674)\nEpoch 8 batch 7 loss: 2.509531021118164 (cur batch: 2.401486396789551)\nEpoch 8 batch 8 loss: 2.5065549744500055 (cur batch: 2.4827466011047363)\nEpoch 8 batch 9 loss: 2.512353610992432 (cur batch: 2.5645413398742676)\nEpoch 8 batch 10 loss: 2.5088631239804355 (cur batch: 2.4739582538604736)\nEpoch 8 batch 11 loss: 2.505135099093119 (cur batch: 2.4641268253326416)\nEpoch 8 batch 12 loss: 2.5111921567183275 (cur batch: 2.583876848220825)\nEpoch 8 batch 13 loss: 2.510372281074524 (cur batch: 2.499713897705078)\nEpoch 8 batch 14 loss: 2.5031952699025473 (cur batch: 2.402717113494873)\nEpoch 8 batch 15 loss: 2.502251982688904 (cur batch: 2.488102674484253)\nEpoch 8 batch 16 loss: 2.499902332530302 (cur batch: 2.462307929992676)\nEpoch 8 batch 17 loss: 2.5022183524237738 (cur batch: 2.541590690612793)\nEpoch 8 batch 18 loss: 2.508406664195814 (cur batch: 2.6197962760925293)\nEpoch 8 batch 19 loss: 2.5054673194885253 (cur batch: 2.449619770050049)\nEpoch 8 batch 20 loss: 2.507078488667806 (cur batch: 2.539301872253418)\nEpoch 8 batch 21 loss: 2.5020013722506436 (cur batch: 2.3953819274902344)\nEpoch 8 batch 22 loss: 2.502810053203417 (cur batch: 2.5206010341644287)\nEpoch 8 batch 23 loss: 2.5031864941120148 (cur batch: 2.5118446350097656)\nEpoch 8 batch 24 loss: 2.5091945362091064 (cur batch: 2.6533875465393066)\nEpoch 8 batch 25 loss: 2.5038880751683164 (cur batch: 2.3712265491485596)\nEpoch 8 batch 26 loss: 2.507571891502098 (cur batch: 2.60335111618042)\nEpoch 8 batch 27 loss: 2.5033161469868253 (cur batch: 2.388411045074463)\nEpoch 8 batch 28 loss: 2.4998146254440834 (cur batch: 2.4017720222473145)\nEpoch 8 batch 29 loss: 2.4963597456614175 (cur batch: 2.3961682319641113)\nEpoch 8 batch 30 loss: 2.4953918764668126 (cur batch: 2.466355800628662)\nEpoch 8 batch 31 loss: 2.4938723742961884 (cur batch: 2.446767807006836)\n\t Validation Loss: 3.0779569149017334\nEpoch 8 batch 32 loss: 2.496408462524414 (cur batch: 2.496408462524414)\nEpoch 8 batch 33 loss: 2.5495781898498535 (cur batch: 2.602747917175293)\nEpoch 8 batch 34 loss: 2.5333107312520347 (cur batch: 2.5007758140563965)\nEpoch 8 batch 35 loss: 2.5302045345306396 (cur batch: 2.520885944366455)\nEpoch 8 batch 36 loss: 2.550083065032959 (cur batch: 2.6295971870422363)\nEpoch 8 batch 37 loss: 2.5339409510294595 (cur batch: 2.453230381011963)\nEpoch 8 batch 38 loss: 2.5325962815965926 (cur batch: 2.5245282649993896)\nEpoch 8 batch 39 loss: 2.5096029341220856 (cur batch: 2.348649501800537)\nEpoch 8 batch 40 loss: 2.5037079652150473 (cur batch: 2.4565482139587402)\nEpoch 8 batch 41 loss: 2.503532814979553 (cur batch: 2.5019564628601074)\nEpoch 8 batch 42 loss: 2.5079375830563633 (cur batch: 2.551985263824463)\nEpoch 8 batch 43 loss: 2.513963282108307 (cur batch: 2.5802459716796875)\nEpoch 8 batch 44 loss: 2.5134251851301928 (cur batch: 2.5069680213928223)\nEpoch 8 batch 45 loss: 2.5050075905663625 (cur batch: 2.3955788612365723)\nEpoch 8 batch 46 loss: 2.5125139077504475 (cur batch: 2.6176023483276367)\nEpoch 8 batch 47 loss: 2.5170066356658936 (cur batch: 2.584397554397583)\nEpoch 8 batch 48 loss: 2.515578606549431 (cur batch: 2.492730140686035)\nEpoch 8 batch 49 loss: 2.5203301906585693 (cur batch: 2.601107120513916)\nEpoch 8 batch 50 loss: 2.5258673366747404 (cur batch: 2.6255359649658203)\nEpoch 8 batch 51 loss: 2.5279752373695374 (cur batch: 2.5680253505706787)\nEpoch 8 batch 52 loss: 2.5282400789714994 (cur batch: 2.533536911010742)\nEpoch 8 batch 53 loss: 2.5302864096381446 (cur batch: 2.5732593536376953)\nEpoch 8 batch 54 loss: 2.525168667668882 (cur batch: 2.4125783443450928)\nEpoch 8 batch 55 loss: 2.523916949828466 (cur batch: 2.4951274394989014)\nEpoch 8 batch 56 loss: 2.520184669494629 (cur batch: 2.430609941482544)\nEpoch 8 batch 57 loss: 2.519354957800645 (cur batch: 2.49861216545105)\nEpoch 8 batch 58 loss: 2.519492511396055 (cur batch: 2.523068904876709)\nEpoch 8 batch 59 loss: 2.520613133907318 (cur batch: 2.550869941711426)\nEpoch 8 batch 60 loss: 2.521571660863942 (cur batch: 2.548410415649414)\nEpoch 8 batch 61 loss: 2.520235013961792 (cur batch: 2.4814722537994385)\nEpoch 8 batch 62 loss: 2.5246111039192445 (cur batch: 2.6558938026428223)\nEpoch 8 batch 63 loss: 2.5193959400057793 (cur batch: 2.3577258586883545)\n\t Validation Loss: 3.1099419593811035\nEpoch 8 batch 64 loss: 2.4680209159851074 (cur batch: 2.4680209159851074)\nEpoch 8 batch 65 loss: 2.4457311630249023 (cur batch: 2.4234414100646973)\nEpoch 8 batch 66 loss: 2.4698707262674966 (cur batch: 2.5181498527526855)\nEpoch 8 batch 67 loss: 2.4621405005455017 (cur batch: 2.4389498233795166)\nEpoch 8 batch 68 loss: 2.462307405471802 (cur batch: 2.462975025177002)\nEpoch 8 batch 69 loss: 2.4588838815689087 (cur batch: 2.4417662620544434)\nEpoch 8 batch 70 loss: 2.45932742527553 (cur batch: 2.461988687515259)\nEpoch 8 batch 71 loss: 2.469709634780884 (cur batch: 2.5423851013183594)\nEpoch 8 batch 72 loss: 2.4700122674306235 (cur batch: 2.47243332862854)\nEpoch 8 batch 73 loss: 2.479888653755188 (cur batch: 2.5687761306762695)\nEpoch 8 batch 74 loss: 2.4907897385683926 (cur batch: 2.5998005867004395)\nEpoch 8 batch 75 loss: 2.494157373905182 (cur batch: 2.5312013626098633)\nEpoch 8 batch 76 loss: 2.4993906938112698 (cur batch: 2.562190532684326)\nEpoch 8 batch 77 loss: 2.503672412463597 (cur batch: 2.5593347549438477)\nEpoch 8 batch 78 loss: 2.5071547667185468 (cur batch: 2.555907726287842)\nEpoch 8 batch 79 loss: 2.498517259955406 (cur batch: 2.368954658508301)\nEpoch 8 batch 80 loss: 2.496182595982271 (cur batch: 2.4588279724121094)\nEpoch 8 batch 81 loss: 2.4955426454544067 (cur batch: 2.484663486480713)\nEpoch 8 batch 82 loss: 2.4970955221276534 (cur batch: 2.5250473022460938)\nEpoch 8 batch 83 loss: 2.497034156322479 (cur batch: 2.49586820602417)\nEpoch 8 batch 84 loss: 2.5007668222699846 (cur batch: 2.5754201412200928)\nEpoch 8 batch 85 loss: 2.499874483455311 (cur batch: 2.481135368347168)\nEpoch 8 batch 86 loss: 2.4983455720155137 (cur batch: 2.464709520339966)\nEpoch 8 batch 87 loss: 2.4999391237894693 (cur batch: 2.536590814590454)\nEpoch 8 batch 88 loss: 2.5015230560302735 (cur batch: 2.5395374298095703)\nEpoch 8 batch 89 loss: 2.501638256586515 (cur batch: 2.5045182704925537)\nEpoch 8 batch 90 loss: 2.498025549782647 (cur batch: 2.40409517288208)\nEpoch 8 batch 91 loss: 2.500331836087363 (cur batch: 2.5626015663146973)\nEpoch 9 batch 0 loss: 2.439314365386963 (cur batch: 2.439314365386963)\nEpoch 9 batch 1 loss: 2.5159175395965576 (cur batch: 2.5925207138061523)\nEpoch 9 batch 2 loss: 2.461887995402018 (cur batch: 2.3538289070129395)\nEpoch 9 batch 3 loss: 2.4543083906173706 (cur batch: 2.4315695762634277)\nEpoch 9 batch 4 loss: 2.4397286891937258 (cur batch: 2.3814098834991455)\nEpoch 9 batch 5 loss: 2.434657335281372 (cur batch: 2.4093005657196045)\nEpoch 9 batch 6 loss: 2.441004582813808 (cur batch: 2.479088068008423)\nEpoch 9 batch 7 loss: 2.4506901502609253 (cur batch: 2.518489122390747)\nEpoch 9 batch 8 loss: 2.449054718017578 (cur batch: 2.435971260070801)\nEpoch 9 batch 9 loss: 2.4639054775238036 (cur batch: 2.597562313079834)\nEpoch 9 batch 10 loss: 2.460176186128096 (cur batch: 2.4228832721710205)\nEpoch 9 batch 11 loss: 2.467227518558502 (cur batch: 2.5447921752929688)\nEpoch 9 batch 12 loss: 2.4676953829251804 (cur batch: 2.4733097553253174)\nEpoch 9 batch 13 loss: 2.4672076191220964 (cur batch: 2.460866689682007)\nEpoch 9 batch 14 loss: 2.4655211130777994 (cur batch: 2.4419100284576416)\nEpoch 9 batch 15 loss: 2.4591014832258224 (cur batch: 2.362807035446167)\nEpoch 9 batch 16 loss: 2.4564176587497486 (cur batch: 2.4134764671325684)\nEpoch 9 batch 17 loss: 2.4587005509270563 (cur batch: 2.497509717941284)\nEpoch 9 batch 18 loss: 2.469940649835687 (cur batch: 2.67226243019104)\nEpoch 9 batch 19 loss: 2.4703195214271547 (cur batch: 2.477518081665039)\nEpoch 9 batch 20 loss: 2.4744720572517034 (cur batch: 2.557522773742676)\nEpoch 9 batch 21 loss: 2.468860138546337 (cur batch: 2.3510098457336426)\nEpoch 9 batch 22 loss: 2.4690205221590786 (cur batch: 2.4725489616394043)\nEpoch 9 batch 23 loss: 2.4640951255957284 (cur batch: 2.350811004638672)\nEpoch 9 batch 24 loss: 2.4609721374511717 (cur batch: 2.3860204219818115)\nEpoch 9 batch 25 loss: 2.4623217032505917 (cur batch: 2.496060848236084)\nEpoch 9 batch 26 loss: 2.468918447141294 (cur batch: 2.6404337882995605)\nEpoch 9 batch 27 loss: 2.469664999416896 (cur batch: 2.4898219108581543)\nEpoch 9 batch 28 loss: 2.472051472499453 (cur batch: 2.538872718811035)\nEpoch 9 batch 29 loss: 2.469954744974772 (cur batch: 2.409149646759033)\nEpoch 9 batch 30 loss: 2.4697962037978636 (cur batch: 2.4650399684906006)\nEpoch 9 batch 31 loss: 2.4719677567481995 (cur batch: 2.539285898208618)\n\t Validation Loss: 3.1821835041046143\nEpoch 9 batch 32 loss: 2.5525131225585938 (cur batch: 2.5525131225585938)\nEpoch 9 batch 33 loss: 2.508760690689087 (cur batch: 2.46500825881958)\nEpoch 9 batch 34 loss: 2.4544199307759604 (cur batch: 2.345738410949707)\nEpoch 9 batch 35 loss: 2.448226571083069 (cur batch: 2.4296464920043945)\nEpoch 9 batch 36 loss: 2.460718584060669 (cur batch: 2.5106866359710693)\nEpoch 9 batch 37 loss: 2.4635717471440635 (cur batch: 2.477837562561035)\nEpoch 9 batch 38 loss: 2.525780814034598 (cur batch: 2.8990352153778076)\nEpoch 9 batch 39 loss: 2.524734139442444 (cur batch: 2.5174074172973633)\nEpoch 9 batch 40 loss: 2.5118684238857694 (cur batch: 2.408942699432373)\nEpoch 9 batch 41 loss: 2.5074837684631346 (cur batch: 2.468021869659424)\nEpoch 9 batch 42 loss: 2.5164378122849898 (cur batch: 2.60597825050354)\nEpoch 9 batch 43 loss: 2.511756638685862 (cur batch: 2.460263729095459)\nEpoch 9 batch 44 loss: 2.5059809501354513 (cur batch: 2.4366726875305176)\nEpoch 9 batch 45 loss: 2.514913865498134 (cur batch: 2.6310417652130127)\nEpoch 9 batch 46 loss: 2.5221559047698974 (cur batch: 2.623544454574585)\nEpoch 9 batch 47 loss: 2.5239018946886063 (cur batch: 2.5500917434692383)\nEpoch 9 batch 48 loss: 2.5284622697269215 (cur batch: 2.601428270339966)\nEpoch 9 batch 49 loss: 2.5254323482513428 (cur batch: 2.473923683166504)\nEpoch 9 batch 50 loss: 2.5150512645119116 (cur batch: 2.3281917572021484)\nEpoch 9 batch 51 loss: 2.506925272941589 (cur batch: 2.3525314331054688)\nEpoch 9 batch 52 loss: 2.509670757112049 (cur batch: 2.5645804405212402)\nEpoch 9 batch 53 loss: 2.500831809910861 (cur batch: 2.315213918685913)\nEpoch 9 batch 54 loss: 2.4972935033881147 (cur batch: 2.4194507598876953)\nEpoch 9 batch 55 loss: 2.504062960545222 (cur batch: 2.6597604751586914)\nEpoch 9 batch 56 loss: 2.5061143016815186 (cur batch: 2.5553464889526367)\nEpoch 9 batch 57 loss: 2.506517162689796 (cur batch: 2.5165886878967285)\nEpoch 9 batch 58 loss: 2.510609573788113 (cur batch: 2.6170122623443604)\nEpoch 9 batch 59 loss: 2.5165259497506276 (cur batch: 2.6762681007385254)\nEpoch 9 batch 60 loss: 2.5152032786402208 (cur batch: 2.478168487548828)\nEpoch 9 batch 61 loss: 2.513183895746867 (cur batch: 2.4546217918395996)\nEpoch 9 batch 62 loss: 2.5114458530179915 (cur batch: 2.4593045711517334)\nEpoch 9 batch 63 loss: 2.5100583657622337 (cur batch: 2.4670462608337402)\n\t Validation Loss: 3.056738297144572\nEpoch 9 batch 64 loss: 2.313197135925293 (cur batch: 2.313197135925293)\nEpoch 9 batch 65 loss: 2.5109455585479736 (cur batch: 2.7086939811706543)\nEpoch 9 batch 66 loss: 2.4869221846262612 (cur batch: 2.438875436782837)\nEpoch 9 batch 67 loss: 2.4820016026496887 (cur batch: 2.4672398567199707)\nEpoch 9 batch 68 loss: 2.478545331954956 (cur batch: 2.4647202491760254)\nEpoch 9 batch 69 loss: 2.4668946266174316 (cur batch: 2.4086410999298096)\nEpoch 9 batch 70 loss: 2.457702398300171 (cur batch: 2.4025490283966064)\nEpoch 9 batch 71 loss: 2.4597300589084625 (cur batch: 2.473923683166504)\nEpoch 9 batch 72 loss: 2.4560464488135443 (cur batch: 2.426577568054199)\nEpoch 9 batch 73 loss: 2.4752277135849 (cur batch: 2.6478590965270996)\nEpoch 9 batch 74 loss: 2.4918622970581055 (cur batch: 2.658208131790161)\nEpoch 9 batch 75 loss: 2.488653381665548 (cur batch: 2.453355312347412)\nEpoch 9 batch 76 loss: 2.4901367334219127 (cur batch: 2.507936954498291)\nEpoch 9 batch 77 loss: 2.4975245680127824 (cur batch: 2.593566417694092)\nEpoch 9 batch 78 loss: 2.49572917620341 (cur batch: 2.4705936908721924)\nEpoch 9 batch 79 loss: 2.49336077272892 (cur batch: 2.4578347206115723)\nEpoch 9 batch 80 loss: 2.4965169990763947 (cur batch: 2.5470166206359863)\nEpoch 9 batch 81 loss: 2.4949474732081094 (cur batch: 2.4682655334472656)\nEpoch 9 batch 82 loss: 2.4980764765488472 (cur batch: 2.554398536682129)\nEpoch 9 batch 83 loss: 2.4925854206085205 (cur batch: 2.3882553577423096)\nEpoch 9 batch 84 loss: 2.4999826749165854 (cur batch: 2.647927761077881)\nEpoch 9 batch 85 loss: 2.4990174770355225 (cur batch: 2.478748321533203)\nEpoch 9 batch 86 loss: 2.497255511905836 (cur batch: 2.4584922790527344)\nEpoch 9 batch 87 loss: 2.502034068107605 (cur batch: 2.611940860748291)\nEpoch 9 batch 88 loss: 2.4997779083251954 (cur batch: 2.4456300735473633)\nEpoch 9 batch 89 loss: 2.4997007755132823 (cur batch: 2.497772455215454)\nEpoch 9 batch 90 loss: 2.5026267104678683 (cur batch: 2.5787010192871094)\nEpoch 9 batch 91 loss: 2.502455004623958 (cur batch: 2.497818946838379)\n",
     "output_type": "stream"
    }
   ],
   "source": [
    "def to_device(inputs, labels):\n",
    "    return inputs.to(device), tuple(map(lambda i: i.to(device), labels))\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl):\n",
    "        inputs, labels = to_device(*data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(inputs)\n",
    "        loss = calc_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print(f'Epoch {epoch} batch {i} loss: {running_loss / (i % config.BATCHES_PER_EVALUATION + 1)} (cur batch: {loss.item()})')\n",
    "        if i % config.BATCHES_PER_EVALUATION == config.BATCHES_PER_EVALUATION-1:\n",
    "            total_val_loss = 0.0\n",
    "            for data in val_dl:\n",
    "                inputs, labels = to_device(*data)\n",
    "                outputs = classifier(inputs)\n",
    "                loss = calc_loss(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "            print(f'\\t Validation Loss: {total_val_loss / len(val_dl)}')\n",
    "            running_loss = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), 'classifier-v1.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}